{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 4D - no collapsing\n",
    "# V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, Bidirectional, AdditiveAttention, LayerNormalization\n",
    "from functools import partial\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image:  (91, 109, 91, 145)\n",
      "shape of image:  (91, 109, 91, 146)\n",
      "Total GAN control loaded: 2\n",
      "Total GAN schiz loaded: 2\n"
     ]
    }
   ],
   "source": [
    "# GAN Training Data Selection\n",
    "gan_train_ids_schiz = random.sample(met_requirement_schizophrenia_ids, 2)\n",
    "gan_test_ids_schiz = [id for id in met_requirement_schizophrenia_ids if id not in gan_train_ids_schiz]\n",
    "\n",
    "gan_train_ids_control = random.sample(met_requirement_control_ids, 2)\n",
    "gan_test_ids_control = [id for id in met_requirement_control_ids if id not in gan_train_ids_control]\n",
    "gan_test_ids_control = random.sample(gan_test_ids_control,2)\n",
    "\n",
    "''' data training for classifier '''\n",
    "''' just use the same train set as GAN above '''\n",
    "\n",
    "# Classifier Test Data Selection\n",
    "classifier_test_ids = gan_test_ids_schiz + gan_test_ids_control\n",
    "\n",
    "''' File loading '''\n",
    "# Specify the directory and file pattern\n",
    "directory_path = '4D/'\n",
    "file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "# Construct the full path pattern\n",
    "path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "matching_files = glob.glob(path_pattern)\n",
    "\n",
    "''' File loading for GAN Training and classifer '''\n",
    "''' But this time we have 2 separate GANs, 1 train on schizoprenia and 1 train on control'''\n",
    "\n",
    "#classifier_image_data = []\n",
    "#classifier_labels = []  # 1 for schizophrenia, 0 for non-schizophrenia\n",
    "gan_image_data_schiz = []\n",
    "gan_image_data_control = []\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in gan_train_ids_schiz:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "        \n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        #t1_data = np.sum(t1_data, axis=1)\n",
    "        print('shape of image: ', t1_data.shape)\n",
    "        gan_image_data_schiz.append(t1_data)\n",
    "\n",
    "    if file_id in gan_train_ids_control:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        #t1_data = np.sum(t1_data, axis=1)\n",
    "        gan_image_data_control.append(t1_data)\n",
    "\n",
    "\n",
    "print(f\"Total GAN control loaded: {len(gan_image_data_control)}\")\n",
    "print(f\"Total GAN schiz loaded: {len(gan_image_data_schiz)}\")\n",
    "\n",
    "\n",
    "\n",
    "'''Determine the maximum time-dimension size '''\n",
    "max_z_size_schiz = max(img.shape[3] for img in gan_image_data_schiz)\n",
    "max_z_size_control = max(img.shape[3] for img in gan_image_data_control)\n",
    "max_t_size = max(max_z_size_schiz,max_z_size_control)\n",
    "\n",
    "\n",
    "# Normalize and pad the data\n",
    "def normalize_and_pad(data, max_t):\n",
    "    normalized = (data - np.min(data)) / (np.max(data) - np.min(data)) * 2 - 1\n",
    "    padded = np.pad(normalized, ((0, 0), (0, 0), (0, 0), (0, max_t - data.shape[3])), mode='constant')\n",
    "    return padded\n",
    "\n",
    "padded_data_schiz = [normalize_and_pad(img, max_t_size) for img in gan_image_data_schiz]\n",
    "padded_data_control = [normalize_and_pad(img, max_t_size) for img in gan_image_data_control]\n",
    "\n",
    "# Convert to numpy arrays and add dummy channel dimension\n",
    "padded_data_array_schiz = padded_data_schiz\n",
    "padded_data_array_control = padded_data_control\n",
    "\n",
    "# Prepare datasets for TensorFlow\n",
    "batch_size = 1\n",
    "# train_dataset_schiz = tf.data.Dataset.from_tensor_slices((padded_data_array_schiz)).shuffle(len(padded_data_array_schiz)).batch(batch_size)\n",
    "# train_dataset_control = tf.data.Dataset.from_tensor_slices((padded_data_array_control)).shuffle(len(padded_data_array_control)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each 3D volume for each time step\n",
    "def flatten_3d_data(data):\n",
    "    return np.reshape(data, (data.shape[0], data.shape[1] * data.shape[2] * data.shape[3]))\n",
    "\n",
    "# Prepare the data for the LSTM model\n",
    "flattened_data_schiz = [flatten_3d_data(img) for img in padded_data_array_schiz]\n",
    "flattened_data_control = [flatten_3d_data(img) for img in padded_data_array_control]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_images_schiz = np.array(flattened_data_schiz)\n",
    "train_images_control = np.array(flattened_data_control)\n",
    "\n",
    "\n",
    "# Create labels\n",
    "labels_schiz = np.ones(len(padded_data_array_schiz))\n",
    "labels_control = np.zeros(len(padded_data_array_control))\n",
    "\n",
    "# Combine the datasets\n",
    "train_images = np.concatenate((train_images_schiz, train_images_control), axis=0)\n",
    "train_labels = np.concatenate((labels_schiz, labels_control), axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(train_images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_images = train_images[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[902629,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 26\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_rnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m rnn_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mbuild_rnn_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# LSTM layers to process the sequence\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#model.add(layers.Dropout(0.5))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Output layer for binary classification\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[902629,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "def build_rnn_model():\n",
    "    \n",
    "    # Define input shape: (time_steps, flattened_features)\n",
    "    time_steps = max_t_size  # Number of time points\n",
    "    features_per_volume = 91 * 109 * 91  # Flattened size of each 3D volume\n",
    "\n",
    "    # Input shape for LSTM\n",
    "    input_shape = (time_steps, features_per_volume)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "\n",
    "    # LSTM layers to process the sequence\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.LSTM(64))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer for binary classification\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "rnn_model = build_rnn_model()\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Set mixed precision policy to reduce memory usage\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Control GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 15\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        images = tf.transpose(images, perm=[0, 4, 1, 2, 3, 5])  # (batch, t, x, y, z, channels)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = rnn_model(images, training=True)\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels, tf.squeeze(predictions, axis=1))\n",
    "        \n",
    "        gradients = tape.gradient(loss, rnn_model.trainable_variables)\n",
    "        rnn_model.optimizer.apply_gradients(zip(gradients, rnn_model.trainable_variables))\n",
    "        \n",
    "        # Update the metrics\n",
    "        train_loss.update_state(loss)\n",
    "        train_accuracy.update_state(labels, tf.squeeze(predictions, axis=1))\n",
    "    \n",
    "    # Print the loss and accuracy for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {train_loss.result().numpy()}, Training Accuracy: {train_accuracy.result().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0] / image.shape[0],\n",
    "        new_shape[1] / image.shape[1],\n",
    "        new_shape[2] / image.shape[2]\n",
    "    )\n",
    "    return scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation\n",
    "\n",
    "\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "test_ids = classifier_test_ids  # List of IDs to filter test data\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in test_ids:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "        \n",
    "        t1_data_collapsed = np.sum(t1_data, axis=1)  # Summing over one axis\n",
    "\n",
    "        # Resize the collapsed data\n",
    "        processed_image = resize_image(t1_data_collapsed, (91, 91, 146))\n",
    "\n",
    "        # Normalize the processed image\n",
    "        processed_image_normalized = (processed_image - np.min(processed_image)) / (np.max(processed_image) - np.min(processed_image)) * 2 - 1\n",
    "\n",
    "        # Reshape for RNN input: Flatten each 91x91 image to a single vector and treat each as a timestep\n",
    "        processed_image_flattened = np.reshape(processed_image_normalized, (146, 91*91))\n",
    "\n",
    "        test_image_data.append(processed_image_flattened)\n",
    "        label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays for easier handling in TensorFlow\n",
    "test_images_array = np.array(test_image_data)\n",
    "test_labels_array = np.array(test_labels)\n",
    "\n",
    "# Create a TensorFlow dataset from the numpy arrays\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = rnn_model.evaluate(test_dataset)\n",
    "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities\n",
    "predictions = rnn_model.predict(test_dataset)\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Assuming test_labels_array contains your actual labels\n",
    "actual_labels = test_labels_array\n",
    "\n",
    "# Now you might want to compare these predicted_labels with the actual labels (test_labels)\n",
    "# to compute the confusion matrix, classification report, etc.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(actual_labels, predicted_labels.flatten()))\n",
    "print(classification_report(actual_labels, predicted_labels.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actual, predicted in zip(actual_labels, predicted_labels):\n",
    "    print(f'Actual: {actual}, Predicted: {predicted}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
