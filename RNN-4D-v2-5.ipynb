{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 4D - no collapsing\n",
    "# V2.5 - run on lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, Bidirectional, AdditiveAttention, LayerNormalization\n",
    "from functools import partial\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nibabel as nib\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Directory containing your .nii.gz files\n",
    "# directory_path = '4D/'\n",
    "# file_pattern = '*.nii.gz'  # Adjust as needed for your file pattern\n",
    "\n",
    "# # Directory to move corrupt files, create if doesn't exist\n",
    "# corrupt_files_dir = os.path.join(directory_path, 'corrupt_files')\n",
    "# os.makedirs(corrupt_files_dir, exist_ok=True)\n",
    "\n",
    "# # List to store paths of corrupt files\n",
    "# corrupt_files = []\n",
    "\n",
    "# # Iterate over all files in the directory\n",
    "# for root, _, files in os.walk(directory_path):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.nii.gz'):\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             try:\n",
    "#                 # Attempt to load the file\n",
    "#                 t1_img = nib.load(file_path)\n",
    "#                 # Attempt to read the data to ensure it's not corrupt\n",
    "#                 t1_data = t1_img.get_fdata()\n",
    "\n",
    "#             except (EOFError, OSError, nib.filebasedimages.ImageFileError) as e:\n",
    "#                 # Log the corrupt file and the error message\n",
    "#                 print(f\"Corrupt file detected: {file_path} | Error: {e}\")\n",
    "#                 corrupt_files.append(file_path)\n",
    "\n",
    "#                 # Optionally, move the corrupt file to a separate directory\n",
    "#                 shutil.move(file_path, os.path.join(corrupt_files_dir, file))\n",
    "#                 continue\n",
    "\n",
    "# # Output the list of corrupt files\n",
    "# if corrupt_files:\n",
    "#     print(f\"\\nTotal corrupt files found: {len(corrupt_files)}\")\n",
    "#     for corrupt_file in corrupt_files:\n",
    "#         print(corrupt_file)\n",
    "# else:\n",
    "#     print(\"No corrupt files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GAN control loaded: 5\n",
      "Total GAN schiz loaded: 5\n",
      "shape after normalization and padding (91, 109, 91, 146)\n"
     ]
    }
   ],
   "source": [
    "# GAN Training Data Selection\n",
    "gan_train_ids_schiz = random.sample(met_requirement_schizophrenia_ids, 5)\n",
    "gan_test_ids_schiz = [id for id in met_requirement_schizophrenia_ids if id not in gan_train_ids_schiz]\n",
    "\n",
    "gan_train_ids_control = random.sample(met_requirement_control_ids, 5)\n",
    "gan_test_ids_control = [id for id in met_requirement_control_ids if id not in gan_train_ids_control]\n",
    "gan_test_ids_control = random.sample(gan_test_ids_control,9)\n",
    "\n",
    "''' data training for classifier '''\n",
    "''' just use the same train set as GAN above '''\n",
    "\n",
    "# Classifier Test Data Selection\n",
    "classifier_test_ids = gan_test_ids_schiz + gan_test_ids_control\n",
    "\n",
    "''' File loading '''\n",
    "# Specify the directory and file pattern\n",
    "directory_path = '4D/'\n",
    "file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "# Construct the full path pattern\n",
    "path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "matching_files = glob.glob(path_pattern)\n",
    "\n",
    "''' File loading for GAN Training and classifer '''\n",
    "''' But this time we have 2 separate GANs, 1 train on schizoprenia and 1 train on control'''\n",
    "\n",
    "#classifier_image_data = []\n",
    "#classifier_labels = []  # 1 for schizophrenia, 0 for non-schizophrenia\n",
    "gan_image_data_schiz = []\n",
    "gan_image_data_control = []\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in gan_train_ids_schiz:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "        \n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        #t1_data = np.sum(t1_data, axis=1)\n",
    "        #print('shape of image: ', t1_data.shape)\n",
    "        gan_image_data_schiz.append(t1_data)\n",
    "\n",
    "    if file_id in gan_train_ids_control:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        #t1_data = np.sum(t1_data, axis=1)\n",
    "        gan_image_data_control.append(t1_data)\n",
    "\n",
    "\n",
    "print(f\"Total GAN control loaded: {len(gan_image_data_control)}\")\n",
    "print(f\"Total GAN schiz loaded: {len(gan_image_data_schiz)}\")\n",
    "\n",
    "\n",
    "\n",
    "'''Determine the maximum time-dimension size '''\n",
    "max_z_size_schiz = max(img.shape[3] for img in gan_image_data_schiz)\n",
    "max_z_size_control = max(img.shape[3] for img in gan_image_data_control)\n",
    "max_t_size = max(max_z_size_schiz,max_z_size_control)\n",
    "\n",
    "\n",
    "# Normalize and pad the data\n",
    "def normalize_and_pad(data, max_t):\n",
    "    normalized = (data - np.min(data)) / (np.max(data) - np.min(data)) * 2 - 1\n",
    "    padded = np.pad(normalized, ((0, 0), (0, 0), (0, 0), (0, max_t - data.shape[3])), mode='constant')\n",
    "    return padded\n",
    "\n",
    "padded_data_schiz = [normalize_and_pad(img, max_t_size) for img in gan_image_data_schiz]\n",
    "padded_data_control = [normalize_and_pad(img, max_t_size) for img in gan_image_data_control]\n",
    "\n",
    "padded_data_array_schiz = padded_data_schiz\n",
    "padded_data_array_control = padded_data_control\n",
    "print(\"shape after normalization and padding\", padded_data_array_control[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for GPUs.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth for GPUs.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after flattening: (146, 902629)\n",
      "Batch image shape: (2, 146, 902629)\n",
      "Batch labels shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Function to flatten 3D volumes for each time step (t, x*y*z)\n",
    "def flatten_3d_data(data):\n",
    "    return np.reshape(data, (data.shape[3], data.shape[0] * data.shape[1] * data.shape[2]))  # (t, x*y*z)\n",
    "\n",
    "# Flatten each 3D volume for each time step\n",
    "flattened_data_schiz = [flatten_3d_data(img) for img in padded_data_array_schiz]\n",
    "flattened_data_control = [flatten_3d_data(img) for img in padded_data_array_control]\n",
    "print(\"Shape after flattening:\", flattened_data_schiz[0].shape)  # Check the shape after flattening\n",
    "\n",
    "# Create labels\n",
    "labels_schiz = np.ones(len(flattened_data_schiz))\n",
    "labels_control = np.zeros(len(flattened_data_control))\n",
    "\n",
    "# Combine images and labels\n",
    "train_images = flattened_data_schiz + flattened_data_control\n",
    "train_labels = np.concatenate((labels_schiz, labels_control), axis=0)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.arange(len(train_images))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle data based on indices\n",
    "train_images = [train_images[i] for i in indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Define a generator function to yield data batches\n",
    "def data_generator(images, labels, batch_size):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "# Create TensorFlow Dataset from the generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_images, train_labels, batch_size),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, max_t_size, 91 * 109 * 91), dtype=tf.float32),  # Adjust shape if needed\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prefetch for performance improvement\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Debug: Test the generator\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSTA-LSTM uses spatial attention to focus on important regions in each 3D volume \\nand temporal attention to prioritize critical time steps. \\nThe dual attention mechanism is highly effective for data with both spatial and temporal components.\\nDirectly learning spatial and temporal features.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "STA-LSTM uses spatial attention to focus on important regions in each 3D volume \n",
    "and temporal attention to prioritize critical time steps. \n",
    "The dual attention mechanism is highly effective for data with both spatial and temporal components.\n",
    "Directly learning spatial and temporal features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: ['/physical_device:GPU:0', '/physical_device:GPU:1', '/physical_device:GPU:2', '/physical_device:GPU:3', '/physical_device:GPU:4', '/physical_device:GPU:5', '/physical_device:GPU:6', '/physical_device:GPU:7']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 146, 902629)]        0         []                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 146, 91, 109, 91)     0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 146, 902629)          0         ['reshape[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 146, 902629)          9251954   ['time_distributed[0][0]',    \n",
      " iHeadAttention)                                          93         'time_distributed[0][0]']    \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 146, 128)             4621793   ['multi_head_attention[0][0]']\n",
      " al)                                                      28                                      \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 146, 128)             263808    ['bidirectional[0][0]',       \n",
      " ltiHeadAttention)                                                   'bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 64)                   49408     ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    65        ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1387688102 (5.17 GB)\n",
      "Trainable params: 1387688102 (5.17 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Collective all_reduce tensors: 27 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 27 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1, Loss: 0.8633553385734558, Training Accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 2, Loss: 0.7680607438087463, Training Accuracy: 0.20000000298023224\n",
      "Epoch 3, Loss: 0.7107483148574829, Training Accuracy: 0.6000000238418579\n",
      "Epoch 4, Loss: 0.6977447271347046, Training Accuracy: 0.6000000238418579\n",
      "Epoch 5, Loss: 0.690276026725769, Training Accuracy: 0.6000000238418579\n",
      "Epoch 6, Loss: 0.6862671971321106, Training Accuracy: 0.6000000238418579\n",
      "Epoch 7, Loss: 0.6852225065231323, Training Accuracy: 0.6000000238418579\n",
      "Epoch 8, Loss: 0.6851054430007935, Training Accuracy: 0.6000000238418579\n",
      "Epoch 9, Loss: 0.6850528120994568, Training Accuracy: 0.6000000238418579\n",
      "Epoch 10, Loss: 0.6847376823425293, Training Accuracy: 0.6000000238418579\n",
      "Epoch 11, Loss: 0.6842223405838013, Training Accuracy: 0.6000000238418579\n",
      "Epoch 12, Loss: 0.6836258769035339, Training Accuracy: 0.6000000238418579\n",
      "Epoch 13, Loss: 0.6830676794052124, Training Accuracy: 0.6000000238418579\n",
      "Epoch 14, Loss: 0.6826241612434387, Training Accuracy: 0.6000000238418579\n",
      "Epoch 15, Loss: 0.682268500328064, Training Accuracy: 0.6000000238418579\n",
      "Epoch 16, Loss: 0.6820175647735596, Training Accuracy: 0.6000000238418579\n",
      "Epoch 17, Loss: 0.6818077564239502, Training Accuracy: 0.6000000238418579\n",
      "Epoch 18, Loss: 0.6816467642784119, Training Accuracy: 0.6000000238418579\n",
      "Epoch 19, Loss: 0.6814557313919067, Training Accuracy: 0.6000000238418579\n",
      "Epoch 20, Loss: 0.6812615394592285, Training Accuracy: 0.6000000238418579\n",
      "Epoch 21, Loss: 0.6810817718505859, Training Accuracy: 0.6000000238418579\n",
      "Epoch 22, Loss: 0.6808894872665405, Training Accuracy: 0.6000000238418579\n",
      "Epoch 23, Loss: 0.680698812007904, Training Accuracy: 0.6000000238418579\n",
      "Epoch 24, Loss: 0.6805336475372314, Training Accuracy: 0.6000000238418579\n",
      "Epoch 25, Loss: 0.6803722381591797, Training Accuracy: 0.6000000238418579\n",
      "Epoch 26, Loss: 0.6802115440368652, Training Accuracy: 0.6000000238418579\n",
      "Epoch 27, Loss: 0.6800709962844849, Training Accuracy: 0.6000000238418579\n",
      "Epoch 28, Loss: 0.6799284815788269, Training Accuracy: 0.6000000238418579\n",
      "Epoch 29, Loss: 0.6798087954521179, Training Accuracy: 0.6000000238418579\n",
      "Epoch 30, Loss: 0.679669201374054, Training Accuracy: 0.6000000238418579\n",
      "Epoch 31, Loss: 0.6795276999473572, Training Accuracy: 0.6000000238418579\n",
      "Epoch 32, Loss: 0.6793984174728394, Training Accuracy: 0.6000000238418579\n",
      "Epoch 33, Loss: 0.6792770624160767, Training Accuracy: 0.6000000238418579\n",
      "Epoch 34, Loss: 0.6791690587997437, Training Accuracy: 0.6000000238418579\n",
      "Epoch 35, Loss: 0.6790684461593628, Training Accuracy: 0.6000000238418579\n",
      "Epoch 36, Loss: 0.6789688467979431, Training Accuracy: 0.6000000238418579\n",
      "Epoch 37, Loss: 0.6788794994354248, Training Accuracy: 0.6000000238418579\n",
      "Epoch 38, Loss: 0.6787948608398438, Training Accuracy: 0.6000000238418579\n",
      "Epoch 39, Loss: 0.6787028908729553, Training Accuracy: 0.6000000238418579\n",
      "Epoch 40, Loss: 0.6786327362060547, Training Accuracy: 0.6000000238418579\n",
      "Epoch 41, Loss: 0.6785404086112976, Training Accuracy: 0.6000000238418579\n",
      "Epoch 42, Loss: 0.6784705519676208, Training Accuracy: 0.6000000238418579\n",
      "Epoch 43, Loss: 0.6783968806266785, Training Accuracy: 0.6000000238418579\n",
      "Epoch 44, Loss: 0.6783138513565063, Training Accuracy: 0.6000000238418579\n",
      "Epoch 45, Loss: 0.6782516241073608, Training Accuracy: 0.6000000238418579\n",
      "Epoch 46, Loss: 0.6781989336013794, Training Accuracy: 0.6000000238418579\n",
      "Epoch 47, Loss: 0.6781238913536072, Training Accuracy: 0.6000000238418579\n",
      "Epoch 48, Loss: 0.678057074546814, Training Accuracy: 0.6000000238418579\n",
      "Epoch 49, Loss: 0.6779997944831848, Training Accuracy: 0.6000000238418579\n",
      "Epoch 50, Loss: 0.6779425740242004, Training Accuracy: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "# Parallelize\n",
    "# Check available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Available GPUs: {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"No GPUs detected.\")\n",
    "\n",
    "# Initialize MirroredStrategy to utilize all available GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()  # Automatically detects and uses all GPUs\n",
    "\n",
    "# Wrap model creation and training code within the strategy scope\n",
    "with strategy.scope():\n",
    "    def build_rnn_model():\n",
    "        # Define input shape: (time_steps, flattened_features)\n",
    "        time_steps = max_t_size  # Number of time points\n",
    "        features_per_volume = 91 * 109 * 91  # Flattened size of each 3D volume\n",
    "        input_shape = (time_steps, features_per_volume)\n",
    "\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "        # Reshape to prepare for spatial attention\n",
    "        reshaped = layers.Reshape((time_steps, 91, 109, 91))(inputs)  # Reshape to (t, x, y, z)\n",
    "\n",
    "        # Apply spatial attention: Flatten each 3D volume and use attention\n",
    "        spatial_flatten = layers.TimeDistributed(layers.Flatten())(reshaped)\n",
    "        spatial_attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)(spatial_flatten, spatial_flatten)\n",
    "\n",
    "        # Bidirectional LSTM layers to process the attended spatial features\n",
    "        lstm_out = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(spatial_attention)\n",
    "\n",
    "        # Apply temporal attention on the LSTM outputs\n",
    "        temporal_attention = layers.MultiHeadAttention(num_heads=4, key_dim=128)(lstm_out, lstm_out)\n",
    "\n",
    "        # Final LSTM layer to integrate attended features\n",
    "        final_lstm_out = layers.LSTM(64)(temporal_attention)\n",
    "\n",
    "        # Output layer for binary classification\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(final_lstm_out)\n",
    "\n",
    "        # Compile model\n",
    "        model = models.Model(inputs, outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    rnn_model = build_rnn_model()\n",
    "    rnn_model.summary()\n",
    "\n",
    "    # Metrics for training\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "    # Define number of epochs\n",
    "    epochs = 50\n",
    "\n",
    "    # Define a step function for distributed training\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = rnn_model(images, training=True)\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels, tf.squeeze(predictions, axis=1))\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, rnn_model.trainable_variables)\n",
    "\n",
    "        # Apply gradients across all replicas\n",
    "        rnn_model.optimizer.apply_gradients(zip(gradients, rnn_model.trainable_variables))\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss.update_state(loss)\n",
    "        train_accuracy.update_state(labels, tf.squeeze(predictions, axis=1))\n",
    "\n",
    "    # Training loop using the existing train_dataset\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        # Iterate through the training dataset using strategy.run()\n",
    "        for images, labels in train_dataset:\n",
    "            # Use strategy.run to ensure that each step runs in the correct replica context\n",
    "            strategy.run(train_step, args=(images, labels))\n",
    "\n",
    "        # Print the loss and accuracy for the current epoch\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss.result().numpy()}, Training Accuracy: {train_accuracy.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 05:24:07.256265: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 61.86GiB (rounded to 66419052544)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-09-10 05:24:07.256659: W tensorflow/tsl/framework/bfc_allocator.cc:497] _________________________________________________****_____________________**************************\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50036/1786846355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Create a TensorFlow dataset from the numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 133\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    134\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m   1444\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    322\u001b[0m                                          as_ref=False):\n\u001b[1;32m    323\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;31m# Register the conversion function for the \"unconvertible\" types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0] / image.shape[0],\n",
    "        new_shape[1] / image.shape[1],\n",
    "        new_shape[2] / image.shape[2]\n",
    "    )\n",
    "    return scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation\n",
    "\n",
    "\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "test_ids = classifier_test_ids  # List of IDs to filter test data\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in test_ids:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "    \n",
    "        # Normalize the processed image\n",
    "        processed_image_normalized = (t1_data - np.min(t1_data)) / (np.max(t1_data) - np.min(t1_data)) * 2 - 1\n",
    "\n",
    "        # Pad or truncate the time dimension to match the expected size (max_t_size)\n",
    "        current_t_size = processed_image_normalized.shape[3]\n",
    "        \n",
    "        if current_t_size < max_t_size:\n",
    "            # Pad to max_t_size time steps if the current time dimension is smaller\n",
    "            pad_size = max_t_size - current_t_size\n",
    "            #pad_size = max_t_size\n",
    "            processed_image_padded = np.pad(processed_image_normalized, ((0, 0), (0, 0), (0, 0), (0, pad_size)), mode='constant')\n",
    "        elif current_t_size > max_t_size:\n",
    "            # Truncate to max_t_size time steps if the current time dimension is larger\n",
    "            processed_image_padded = processed_image_normalized[:, :, :, :max_t_size]\n",
    "        else:\n",
    "            processed_image_padded = processed_image_normalized  # No padding needed\n",
    "\n",
    "        # Check final shape before flattening\n",
    "        #print(f\"Processed image padded shape: {processed_image_padded.shape}\")\n",
    "\n",
    "        # Flatten each 3D volume (91, 109, 91) for each time step (t)\n",
    "        flattened_shape = (processed_image_padded.shape[3], processed_image_padded.shape[0] * processed_image_padded.shape[1] * processed_image_padded.shape[2])\n",
    "        processed_image_flattened = np.reshape(processed_image_padded, flattened_shape)  # (t, x*y*z)\n",
    "\n",
    "        # Verify the shape after flattening\n",
    "        #print(f\"Flattened image shape: {processed_image_flattened.shape}\")\n",
    "\n",
    "        test_image_data.append(processed_image_flattened)\n",
    "        \n",
    "        label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays for easier handling in TensorFlow\n",
    "test_images_array = np.array(test_image_data)\n",
    "test_labels_array = np.array(test_labels)\n",
    "\n",
    "# Check the final shape of test images\n",
    "#print(\"Final shape of test_images_array:\", test_images_array.shape)\n",
    "#print(\"Final shape of test_labels_array:\", test_labels_array.shape)\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# Create a TensorFlow dataset from the numpy arrays\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:Error reported to Coordinator: Exception encountered when calling layer 'forward_lstm' (type LSTM).\n",
      "\n",
      "{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:1}} CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1694): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)' [Op:CudnnRNN]\n",
      "\n",
      "Call arguments received by layer 'forward_lstm' (type LSTM):\n",
      "  • inputs=tf.Tensor(shape=(0, 146, 902629), dtype=float32)\n",
      "  • mask=None\n",
      "  • training=False\n",
      "  • initial_state=None\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 387, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py\", line 596, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1944, in run_step\n",
      "    outputs = model.test_step(data)\n",
      "  File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1850, in test_step\n",
      "    y_pred = self(x, training=False)\n",
      "  File \"/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Exception encountered when calling layer 'forward_lstm' (type LSTM).\n",
      "\n",
      "{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:1}} CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1694): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)' [Op:CudnnRNN]\n",
      "\n",
      "Call arguments received by layer 'forward_lstm' (type LSTM):\n",
      "  • inputs=tf.Tensor(shape=(0, 146, 902629), dtype=float32)\n",
      "  • mask=None\n",
      "  • training=False\n",
      "  • initial_state=None\n",
      "An error occurred during evaluation: Exception encountered when calling layer 'forward_lstm' (type LSTM).\n",
      "\n",
      "{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:1}} CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1694): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)' [Op:CudnnRNN]\n",
      "\n",
      "Call arguments received by layer 'forward_lstm' (type LSTM):\n",
      "  • inputs=tf.Tensor(shape=(0, 146, 902629), dtype=float32)\n",
      "  • mask=None\n",
      "  • training=False\n",
      "  • initial_state=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 05:37:59.256729: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at cudnn_rnn_ops.cc:1769 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1694): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n"
     ]
    }
   ],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0] / image.shape[0],\n",
    "        new_shape[1] / image.shape[1],\n",
    "        new_shape[2] / image.shape[2]\n",
    "    )\n",
    "    return scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation\n",
    "\n",
    "\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "test_ids = classifier_test_ids  # List of IDs to filter test data\n",
    "\n",
    "# Test data processing with generator\n",
    "def test_data_generator():\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "            \n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Normalize the processed image\n",
    "            processed_image_normalized = (t1_data - np.min(t1_data)) / (np.max(t1_data) - np.min(t1_data)) * 2 - 1\n",
    "\n",
    "            # Pad or truncate the time dimension to match max_t_size\n",
    "            current_t_size = processed_image_normalized.shape[3]\n",
    "            if current_t_size < max_t_size:\n",
    "                pad_size = max_t_size - current_t_size\n",
    "                processed_image_padded = np.pad(processed_image_normalized, ((0, 0), (0, 0), (0, 0), (0, pad_size)), mode='constant')\n",
    "            elif current_t_size > max_t_size:\n",
    "                processed_image_padded = processed_image_normalized[:, :, :, :max_t_size]\n",
    "            else:\n",
    "                processed_image_padded = processed_image_normalized\n",
    "\n",
    "            # Flatten each 3D volume\n",
    "            flattened_shape = (processed_image_padded.shape[3], processed_image_padded.shape[0] * processed_image_padded.shape[1] * processed_image_padded.shape[2])\n",
    "            processed_image_flattened = np.reshape(processed_image_padded, flattened_shape)  # (t, x*y*z)\n",
    "\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "            yield processed_image_flattened, label\n",
    "\n",
    "# Define the output signature for TensorFlow dataset\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(max_t_size, 91 * 109 * 91), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "# Create the dataset from the generator\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    test_data_generator,\n",
    "    output_signature=output_signature\n",
    ").batch(batch_size)\n",
    "\n",
    "# Disable sharding for evaluation\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "test_dataset = test_dataset.with_options(options)\n",
    "\n",
    "# Run evaluation on CPU to manage memory and avoid GPU-specific issues\n",
    "@tf.function\n",
    "def evaluate_on_cpu():\n",
    "    with tf.device('/CPU:0'):\n",
    "        # Disable CuDNN for LSTM layers if needed to avoid GPU memory issues\n",
    "        for layer in rnn_model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.LSTM):\n",
    "                layer._could_use_cudnn = False  # Forces TensorFlow to use CPU-friendly LSTM\n",
    "\n",
    "        # Evaluate the model on the test dataset\n",
    "        try:\n",
    "            loss, accuracy = rnn_model.evaluate(test_dataset)\n",
    "            print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {e}\")\n",
    "\n",
    "# Run the evaluation function\n",
    "evaluate_on_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities\n",
    "predictions = rnn_model.predict(test_dataset)\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Assuming test_labels_array contains your actual labels\n",
    "actual_labels = test_labels_array\n",
    "\n",
    "# Now you might want to compare these predicted_labels with the actual labels (test_labels)\n",
    "# to compute the confusion matrix, classification report, etc.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(actual_labels, predicted_labels.flatten()))\n",
    "print(classification_report(actual_labels, predicted_labels.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actual, predicted in zip(actual_labels, predicted_labels):\n",
    "    print(f'Actual: {actual}, Predicted: {predicted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
