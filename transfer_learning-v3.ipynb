{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try transfer learning using WGAN critic/discriminator\n",
    "# V3\n",
    "\n",
    "adapt from here https://github.com/ck44liu/gans-on-image-classification/blob/main/report%20gan%20assignment.pdf\n",
    "\n",
    "if this doesn't work then try this next, https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/\n",
    "\n",
    "https://www.mdpi.com/2227-7390/10/9/1541\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "critic = load_model('wgan_critic_model_v3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freeze the original layers of the model to retain the learned features before adding classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 84, 84, 72, 1)]   0         \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 42, 42, 36, 64)    1792      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 42, 42, 36, 64)    0         \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 21, 21, 18, 128)   221312    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 21, 21, 18, 128)   0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1016064)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               130056320 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,279,553\n",
      "Trainable params: 130,279,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "new_model = Sequential()\n",
    "for i in range(len(critic.layers)-1):  # Excluding the last layer\n",
    "    layer = critic.layers[i]\n",
    "    layer.trainable = False  # Freeze the layer\n",
    "    new_model.add(layer)\n",
    "\n",
    "# Adding additional layers\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Dense(128, activation='relu', name = 'dense_9'))\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(64, activation='relu', name = 'dense_5'))\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(1, activation='sigmoid', name = 'dense_6'))  # Sigmoid for binary classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 42, 42, 36, 64)    1792      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 42, 42, 36, 64)    0         \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 21, 21, 18, 128)   221312    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 21, 21, 18, 128)   0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1016064)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               130056320 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,304,769\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 130,279,680\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since we have only one dataset, split it into training and evaluation parts. \n",
    "\n",
    "## for classification training, use 50 control and 50 schizphrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files successfully processed: 74\n",
      "Total number of schizophrenia files: 34\n",
      "Schizophrenia files: ['A00000456_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00000838_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00001251_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00006754_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00012767_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00013216_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014607_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014804_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014830_0010_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00015648_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00016197_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00016720_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00016723_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00017147_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00018434_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00018979_0020_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00019293_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00020414_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00020787_0017_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00021598_0010_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023590_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023750_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024228_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024953_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024959_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00027537_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00028303_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00028806_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00031597_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00034273_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00035003_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00035485_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037224_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00038624_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz']\n",
      "Total number of non-schizophrenia files: 40\n",
      "Non-Schizophrenia files: ['A00002198_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00004087_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00007409_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00011265_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00012995_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014839_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00015759_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00015826_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00018553_0010_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00019888_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00020805_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00020968_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00021081_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022509_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022592_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022727_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022729_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022835_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022837_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00022915_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023131_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023730_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023800_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023848_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024372_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024446_0009_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024535_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024663_0016_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024932_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00024955_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00027487_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00027787_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00028406_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00028409_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00029452_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00036049_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037007_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037495_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037564_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037665_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz']\n",
      "Total number of files with insufficient time dimension: 28\n",
      "Files with insufficient time dimension: ['A00000909_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00001181_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00001243_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00002480_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00010150_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00013363_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014120_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014175_0010_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014225_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014590_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00014636_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00015518_0014_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00017294_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00019349_0011_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00023243_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00026907_0013_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00027391_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00027755_0015_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00028405_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00029486_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00031271_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00033648_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00033812_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00033994_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00035183_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00036897_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00036916_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz', 'A00037854_0012_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz']\n",
      "IDs of files with insufficient time dimension: ['A00000909', 'A00001181', 'A00001243', 'A00002480', 'A00010150', 'A00013363', 'A00014120', 'A00014175', 'A00014225', 'A00014590', 'A00014636', 'A00015518', 'A00017294', 'A00019349', 'A00023243', 'A00026907', 'A00027391', 'A00027755', 'A00028405', 'A00029486', 'A00031271', 'A00033648', 'A00033812', 'A00033994', 'A00035183', 'A00036897', 'A00036916', 'A00037854']\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory and file pattern\n",
    "directory_path = '../4D'\n",
    "file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "# Construct the full path pattern\n",
    "path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "matching_files = glob.glob(path_pattern)\n",
    "\n",
    "# Define the list of schizophrenia IDs\n",
    "\n",
    "'''\n",
    "schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',] '''\n",
    "\n",
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "schizophrenia_ids = random.sample(full_schizophrenia_ids, 50)\n",
    "# Define the list of IDs of individuals with non-schizophrenia - control only\n",
    "\n",
    "'''control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667'] '''\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    "control_ids = random.sample(full_control_ids, 50)\n",
    "\n",
    "# Initialize lists to store the processed image data, corresponding labels, and filenames\n",
    "image_data = []\n",
    "labels = []  # 1 for schizophrenia, 0 for non-schizophrenia\n",
    "schizophrenia_files = []\n",
    "non_schizophrenia_files = []\n",
    "\n",
    "# Lists for files with insufficient time dimensions\n",
    "insufficient_time_files = []\n",
    "insufficient_time_ids = []\n",
    "\n",
    "# Counters for each category\n",
    "schizophrenia_count = 0\n",
    "non_schizophrenia_count = 0\n",
    "processed_files_count = 0\n",
    "\n",
    "# Loop through the matching files\n",
    "for file_path in matching_files:\n",
    "    # Extract the filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Extract the ID from the filename\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    # Load the file\n",
    "    t1_img = nib.load(file_path)\n",
    "    t1_data = t1_img.get_fdata()\n",
    "\n",
    "    # Check if the time dimension is at least 90\n",
    "    if t1_data.shape[3] < 90:\n",
    "        insufficient_time_files.append(filename)\n",
    "        insufficient_time_ids.append(file_id)\n",
    "        continue  # Skip this file\n",
    "\n",
    "    # Determine the label based on the ID and increment counters\n",
    "    if file_id in schizophrenia_ids:\n",
    "        label = 1  # Schizophrenia\n",
    "        schizophrenia_count += 1\n",
    "        schizophrenia_files.append(filename)\n",
    "    elif file_id in control_ids:\n",
    "        label = 0  # Non-Schizophrenia\n",
    "        non_schizophrenia_count += 1\n",
    "        non_schizophrenia_files.append(filename)\n",
    "    else:\n",
    "        continue  # Skip files with IDs not in the provided lists\n",
    "    \n",
    "    # Collapse one of the axes by summing\n",
    "    t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "    \n",
    "    # Append the collapsed image data and label to the respective lists\n",
    "    image_data.append(t1_data_collapsed)\n",
    "    labels.append(label)\n",
    "\n",
    "    # Increment the counter\n",
    "    processed_files_count += 1\n",
    "\n",
    "# Print the total number of files processed for each category and their filenames\n",
    "print(f\"Total number of files successfully processed: {processed_files_count}\")\n",
    "print(f\"Total number of schizophrenia files: {schizophrenia_count}\")\n",
    "print(\"Schizophrenia files:\", schizophrenia_files)\n",
    "print(f\"Total number of non-schizophrenia files: {non_schizophrenia_count}\")\n",
    "print(\"Non-Schizophrenia files:\", non_schizophrenia_files)\n",
    "\n",
    "# Print files with insufficient time dimension\n",
    "print(f\"Total number of files with insufficient time dimension: {len(insufficient_time_files)}\")\n",
    "print(\"Files with insufficient time dimension:\", insufficient_time_files)\n",
    "print(\"IDs of files with insufficient time dimension:\", insufficient_time_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(image_data))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0] / image.shape[0],\n",
    "        new_shape[1] / image.shape[1],\n",
    "        new_shape[2] / image.shape[2]\n",
    "    )\n",
    "    # Reshape and add a channel dimension\n",
    "    resized_image = scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation\n",
    "    return np.expand_dims(resized_image, axis=-1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Determine the maximum z-dimension size\n",
    "max_z_size = max(img.shape[2] for img in image_data)\n",
    "image_data_normalized = [(img - np.min(img)) / (np.max(img) - np.min(img)) * 2 - 1 for img in image_data]\n",
    "\n",
    "# Pad each image to have a consistent z-dimension size\n",
    "padded_data = [np.pad(img, ((0, 0), (0, 0), (0, max_z_size - img.shape[2])), mode='constant') for img in image_data_normalized]\n",
    "\n",
    "# Resize each image in the padded data array\n",
    "resized_images = [resize_image(img, (84, 84, 72)) for img in padded_data]\n",
    "\n",
    "# Convert the resized data to a numpy array\n",
    "resized_images_array = np.array(resized_images)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 10\n",
    "\n",
    "# labels array corresponding to the images\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Split the data into training and evaluation sets (80% train, 20% eval)\n",
    "#X_train, X_eval, y_train, y_eval = train_test_split(resized_images_array, labels_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to TensorFlow datasets with labels\n",
    "##train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "#eval_dataset = tf.data.Dataset.from_tensor_slices((X_eval, y_eval)).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try cross validation\n",
    "\n",
    "assess how the results of transferred model generalize to an independent dataset. It provides a better estimate of the model's performance on unseen data compared to a single train-test split.\n",
    "\n",
    "Cross-validation maximizes the use of data by using each data point for both training and validation at different iterations, useful because of our limited number of images\n",
    "\n",
    "Reducing Bias: It minimizes the bias that can occur due to the particular way of splitting the dataset. By rotating the validation set across different parts of the data, we get a more comprehensive view of the model's performance.\n",
    "\n",
    "fine-tune hyperparameters. By evaluating different hyperparameters across various folds, we can identify the settings that work best across multiple subsets of your data.\n",
    "\n",
    "Try stratified k-fold cross-validation. So each fold is a good representative of the whole by having approximately the same percentage of samples of each target class as the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 ...\n",
      "Epoch 1/14\n",
      "6/6 [==============================] - 1s 62ms/step - loss: 0.7490 - accuracy: 0.4576 - val_loss: 1.5077 - val_accuracy: 0.4000\n",
      "Epoch 2/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.6036 - accuracy: 0.6949 - val_loss: 2.3045 - val_accuracy: 0.4000\n",
      "Epoch 3/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.4789 - accuracy: 0.8136 - val_loss: 2.8552 - val_accuracy: 0.4000\n",
      "Epoch 4/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.3428 - accuracy: 0.8983 - val_loss: 3.5231 - val_accuracy: 0.4667\n",
      "Epoch 5/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.3318 - accuracy: 0.8475 - val_loss: 4.2660 - val_accuracy: 0.5333\n",
      "Epoch 6/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.3429 - accuracy: 0.8814 - val_loss: 4.9803 - val_accuracy: 0.6000\n",
      "Epoch 7/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1834 - accuracy: 0.9492 - val_loss: 5.6514 - val_accuracy: 0.6667\n",
      "Epoch 8/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.2133 - accuracy: 0.9153 - val_loss: 6.2038 - val_accuracy: 0.6667\n",
      "Epoch 9/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1938 - accuracy: 0.9492 - val_loss: 6.6249 - val_accuracy: 0.6667\n",
      "Epoch 10/14\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1592 - accuracy: 0.9661 - val_loss: 7.0182 - val_accuracy: 0.7333\n",
      "Epoch 11/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1362 - accuracy: 0.9831 - val_loss: 7.2612 - val_accuracy: 0.7333\n",
      "Epoch 12/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1260 - accuracy: 0.9831 - val_loss: 7.4582 - val_accuracy: 0.7333\n",
      "Epoch 13/14\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.1104 - accuracy: 0.9831 - val_loss: 7.8689 - val_accuracy: 0.7333\n",
      "Epoch 14/14\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.1015 - accuracy: 0.9661 - val_loss: 8.3817 - val_accuracy: 0.7333\n",
      "Training for fold 2 ...\n",
      "Epoch 1/14\n",
      "6/6 [==============================] - 1s 61ms/step - loss: 0.2661 - accuracy: 0.8983 - val_loss: 0.1943 - val_accuracy: 0.9333\n",
      "Epoch 2/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1791 - accuracy: 0.9153 - val_loss: 0.1923 - val_accuracy: 0.9333\n",
      "Epoch 3/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1770 - accuracy: 0.9661 - val_loss: 0.1692 - val_accuracy: 0.9333\n",
      "Epoch 4/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1418 - accuracy: 0.9661 - val_loss: 0.1516 - val_accuracy: 0.9333\n",
      "Epoch 5/14\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1035 - accuracy: 0.9831 - val_loss: 0.1426 - val_accuracy: 0.9333\n",
      "Epoch 6/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1411 - accuracy: 0.9492 - val_loss: 0.1362 - val_accuracy: 0.9333\n",
      "Epoch 7/14\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0878 - accuracy: 0.9831 - val_loss: 0.1216 - val_accuracy: 1.0000\n",
      "Epoch 8/14\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0822 - accuracy: 0.9831 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
      "Epoch 9/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0692 - accuracy: 0.9831 - val_loss: 0.0853 - val_accuracy: 1.0000\n",
      "Epoch 10/14\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 1.0000\n",
      "Epoch 11/14\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0529 - accuracy: 0.9831 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
      "Epoch 12/14\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 13/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0650 - accuracy: 0.9831 - val_loss: 0.0699 - val_accuracy: 1.0000\n",
      "Epoch 14/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0758 - accuracy: 0.9831 - val_loss: 0.0758 - val_accuracy: 1.0000\n",
      "Training for fold 3 ...\n",
      "Epoch 1/14\n",
      "6/6 [==============================] - 1s 61ms/step - loss: 0.1428 - accuracy: 0.9322 - val_loss: 0.0825 - val_accuracy: 1.0000\n",
      "Epoch 2/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0656 - accuracy: 0.9831 - val_loss: 0.0765 - val_accuracy: 1.0000\n",
      "Epoch 3/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 1.0000\n",
      "Epoch 4/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
      "Epoch 5/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
      "Epoch 6/14\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0357 - accuracy: 0.9831 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 7/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
      "Epoch 8/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 9/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 10/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
      "Epoch 11/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
      "Epoch 12/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
      "Epoch 13/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
      "Epoch 14/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 1.0000\n",
      "Training for fold 4 ...\n",
      "Epoch 1/14\n",
      "6/6 [==============================] - 1s 62ms/step - loss: 0.0782 - accuracy: 0.9831 - val_loss: 0.1590 - val_accuracy: 0.9333\n",
      "Epoch 2/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.2266 - val_accuracy: 0.8667\n",
      "Epoch 3/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3142 - val_accuracy: 0.8000\n",
      "Epoch 4/14\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.3589 - val_accuracy: 0.8000\n",
      "Epoch 5/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4071 - val_accuracy: 0.8000\n",
      "Epoch 6/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4872 - val_accuracy: 0.8000\n",
      "Epoch 7/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5520 - val_accuracy: 0.8000\n",
      "Epoch 8/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5909 - val_accuracy: 0.8000\n",
      "Epoch 9/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0221 - accuracy: 0.9831 - val_loss: 0.6049 - val_accuracy: 0.8000\n",
      "Epoch 10/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5658 - val_accuracy: 0.8000\n",
      "Epoch 11/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5523 - val_accuracy: 0.8000\n",
      "Epoch 12/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4778 - val_accuracy: 0.8000\n",
      "Epoch 13/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.8000\n",
      "Epoch 14/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4463 - val_accuracy: 0.8000\n",
      "Training for fold 5 ...\n",
      "Epoch 1/14\n",
      "6/6 [==============================] - 1s 62ms/step - loss: 0.1251 - accuracy: 0.9833 - val_loss: 0.1575 - val_accuracy: 0.9286\n",
      "Epoch 2/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0429 - accuracy: 0.9833 - val_loss: 0.2208 - val_accuracy: 0.9286\n",
      "Epoch 3/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.8571\n",
      "Epoch 4/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.2222 - val_accuracy: 0.8571\n",
      "Epoch 5/14\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9286\n",
      "Epoch 6/14\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9286\n",
      "Epoch 7/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9286\n",
      "Epoch 8/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9286\n",
      "Epoch 9/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0169 - accuracy: 0.9833 - val_loss: 0.2158 - val_accuracy: 0.9286\n",
      "Epoch 10/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 0.9286\n",
      "Epoch 11/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.9286\n",
      "Epoch 12/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9286\n",
      "Epoch 13/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3100 - val_accuracy: 0.9286\n",
      "Epoch 14/14\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9286\n",
      "Average Training Accuracy: 0.9898305058479309\n",
      "Average Validation Accuracy: 0.892380952835083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert the data and labels to numpy arrays\n",
    "X = np.array(resized_images_array)\n",
    "y = np.array(labels_array)\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store metrics\n",
    "fold_no = 1\n",
    "avg_accuracy = 0\n",
    "avg_val_accuracy = 0\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "    # Create TensorFlow datasets for this fold's train and test data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X[train], y[train])).batch(batch_size)\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices((X[test], y[test])).batch(batch_size)\n",
    "\n",
    "    # Compile the model (if using a new instance of the model for each fold)\n",
    "    new_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    history = new_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=14,\n",
    "        validation_data=eval_dataset\n",
    "    )\n",
    "\n",
    "    # Calculating average metrics\n",
    "    avg_accuracy += history.history['accuracy'][-1]\n",
    "    avg_val_accuracy += history.history['val_accuracy'][-1]\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Average metrics across folds\n",
    "avg_accuracy /= n_splits\n",
    "avg_val_accuracy /= n_splits\n",
    "\n",
    "print(f'Average Training Accuracy: {avg_accuracy}')\n",
    "print(f'Average Validation Accuracy: {avg_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.3, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test dataset\n",
    "\n",
    "# Select 5 control and 5 schizophrenia IDs for the test set\n",
    "\n",
    "# Randomly select x schizophrenic and y control IDs\n",
    "schizophrenia_test_ids = random.sample(schizophrenia_ids, 9)\n",
    "control_test_ids = random.sample(control_ids, 9)\n",
    "\n",
    "# Now proceed with loading and preprocessing the images for these IDs\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "# Load and preprocess the images\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "# Combine all test IDs\n",
    "test_ids = control_test_ids + schizophrenia_test_ids\n",
    "\n",
    "# Loop through the matching files and filter based on test IDs\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "\n",
    "    # Process only if the ID is in the test set\n",
    "    if file_id in test_ids:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "\n",
    "        # Ensure sufficient time dimension\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        # Collapse one of the axes by summing\n",
    "        t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "\n",
    "        # Resize, normalize, and add dimension as done in the training data preparation\n",
    "        processed_image = resize_image(t1_data_collapsed, (84, 84, 72))\n",
    "        processed_image_normalized = (processed_image - np.min(processed_image)) / (np.max(processed_image) - np.min(processed_image)) * 2 - 1\n",
    "        processed_image_final = np.expand_dims(processed_image_normalized, axis=-1)\n",
    "\n",
    "        test_image_data.append(processed_image_final)\n",
    "        label = 1 if file_id in schizophrenia_test_ids else 0\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_images_array = np.array(test_image_data)\n",
    "test_labels_array = np.array(test_labels)\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n",
      "Test Loss: 3.2332890033721924, Test Accuracy: 0.5555555820465088\n",
      "Actual labels vs. Predicted labels:\n",
      "Image 1: Actual: 1, Predicted: 0\n",
      "Image 2: Actual: 0, Predicted: 0\n",
      "Image 3: Actual: 0, Predicted: 0\n",
      "Image 4: Actual: 0, Predicted: 0\n",
      "Image 5: Actual: 1, Predicted: 0\n",
      "Image 6: Actual: 0, Predicted: 0\n",
      "Image 7: Actual: 0, Predicted: 0\n",
      "Image 8: Actual: 1, Predicted: 0\n",
      "Image 9: Actual: 1, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics and lists to store results\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "num_batches = 0\n",
    "actual_labels = []\n",
    "predicted_labels_list = []\n",
    "\n",
    "# Manually iterate over the test dataset\n",
    "for images, labels in test_dataset:\n",
    "    # Make predictions\n",
    "    predictions = new_model.predict(images)\n",
    "\n",
    "    # Calculate loss for the batch\n",
    "    loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n",
    "    test_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "    # Process predictions\n",
    "    predicted_labels_batch = tf.cast(tf.round(predictions), dtype=tf.int32)\n",
    "    predicted_labels_list.extend(predicted_labels_batch.numpy().flatten())\n",
    "    actual_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "    # Calculate accuracy for the batch\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predicted_labels_batch), dtype=tf.float32))\n",
    "    test_accuracy += accuracy.numpy()\n",
    "\n",
    "    num_batches += 1\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "test_loss /= num_batches\n",
    "test_accuracy /= num_batches\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "print(\"Actual labels vs. Predicted labels:\")\n",
    "for i in range(len(actual_labels)):\n",
    "    print(f\"Image {i+1}: Actual: {actual_labels[i]}, Predicted: {predicted_labels_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Iteration: 1\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Iteration 1 - Test Loss: 1.0950502334162593, Test Accuracy: 0.7899999916553497\n",
      "Testing Iteration: 2\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Iteration 2 - Test Loss: 2.555393874645233, Test Accuracy: 0.5999999940395355\n",
      "Testing Iteration: 3\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Iteration 3 - Test Loss: 2.564350813627243, Test Accuracy: 0.5949999988079071\n",
      "Testing Iteration: 4\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Iteration 4 - Test Loss: 2.7843761444091797, Test Accuracy: 0.5\n",
      "Testing Iteration: 5\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Iteration 5 - Test Loss: 2.7373902797698975, Test Accuracy: 0.550000011920929\n",
      "Testing Iteration: 6\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Iteration 6 - Test Loss: 2.9793405532836914, Test Accuracy: 0.550000011920929\n",
      "Testing Iteration: 7\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Iteration 7 - Test Loss: 2.3259204626083374, Test Accuracy: 0.5833333432674408\n",
      "Testing Iteration: 8\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Iteration 8 - Test Loss: 2.8268985748291016, Test Accuracy: 0.2800000011920929\n",
      "Testing Iteration: 9\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Iteration 9 - Test Loss: 3.785526990890503, Test Accuracy: 0.5\n",
      "Testing Iteration: 10\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Iteration 10 - Test Loss: 3.006782650947571, Test Accuracy: 0.5166666656732559\n",
      "\n",
      "Average Test Loss over 10 iterations: 2.666103057842702\n",
      "Average Test Accuracy over 10 iterations: 0.5465000018477439\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations to average results\n",
    "num_iterations = 10\n",
    "\n",
    "# Initialize variables to store aggregated results\n",
    "aggregate_test_loss = 0\n",
    "aggregate_test_accuracy = 0\n",
    "\n",
    "# Repeat the testing process for the specified number of iterations\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Testing Iteration: {iteration + 1}\")\n",
    "\n",
    "    # Randomly select x schizophrenic and y control IDs\n",
    "    schizophrenia_test_ids = random.sample(schizophrenia_ids, 9)\n",
    "    control_test_ids = random.sample(control_ids, 9)\n",
    "\n",
    "    # Now proceed with loading and preprocessing the images for these IDs\n",
    "    test_image_data = []\n",
    "    test_labels = []\n",
    "    # Load and preprocess the images\n",
    "    test_image_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Combine all test IDs\n",
    "    test_ids = control_test_ids + schizophrenia_test_ids\n",
    "\n",
    "    # Loop through the matching files and filter based on test IDs\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "\n",
    "        # Process only if the ID is in the test set\n",
    "        if file_id in test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            # Ensure sufficient time dimension\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Collapse one of the axes by summing\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "\n",
    "            # Resize, normalize, and add dimension as done in the training data preparation\n",
    "            processed_image = resize_image(t1_data_collapsed, (84, 84, 72))\n",
    "            processed_image_normalized = (processed_image - np.min(processed_image)) / (np.max(processed_image) - np.min(processed_image)) * 2 - 1\n",
    "            processed_image_final = np.expand_dims(processed_image_normalized, axis=-1)\n",
    "\n",
    "            test_image_data.append(processed_image_final)\n",
    "            label = 1 if file_id in schizophrenia_test_ids else 0\n",
    "            test_labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    test_images_array = np.array(test_image_data)\n",
    "    test_labels_array = np.array(test_labels)\n",
    "\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n",
    "    # Initialize metrics for this iteration\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Manually iterate over the test dataset\n",
    "    for images, labels in test_dataset:\n",
    "        # Make predictions\n",
    "        predictions = new_model.predict(images)\n",
    "\n",
    "        # Calculate loss for the batch\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n",
    "        test_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        predicted_labels_batch = tf.cast(tf.round(predictions), dtype=tf.int32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predicted_labels_batch), dtype=tf.float32))\n",
    "        test_accuracy += accuracy.numpy()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        # Calculate average loss and accuracy for this iteration\n",
    "        test_loss /= num_batches\n",
    "        test_accuracy /= num_batches\n",
    "\n",
    "    # Add to aggregate results\n",
    "    aggregate_test_loss += test_loss\n",
    "    aggregate_test_accuracy += test_accuracy\n",
    "\n",
    "    # Print results for this iteration\n",
    "    print(f\"Iteration {iteration + 1} - Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Calculate and print the average loss and accuracy over all iterations\n",
    "average_test_loss = aggregate_test_loss / num_iterations\n",
    "average_test_accuracy = aggregate_test_accuracy / num_iterations\n",
    "print(f\"\\nAverage Test Loss over {num_iterations} iterations: {average_test_loss}\")\n",
    "print(f\"Average Test Accuracy over {num_iterations} iterations: {average_test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY IS IT KEEP FLUCTUATING?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
