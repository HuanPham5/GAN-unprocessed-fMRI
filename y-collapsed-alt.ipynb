{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN version v3 training + testing all in one file\n",
    "\n",
    "## only schizophrenia\n",
    "\n",
    "### pick only 50 random schizophrenia for training GAN, leave 9 untouch for testing\n",
    "\n",
    "### When training 2nd time for classification, also pick 50 random control, leave 9 untouch for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 03:52:10.191928: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-31 03:52:10.228816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-31 03:52:10.228846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-31 03:52:10.229855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-31 03:52:10.236725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 03:52:10.817580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc65d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices of type 'GPU'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f'Number of GPUs available: {len(gpus)}')\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f'GPU {i}: {gpu}')\n",
    "else:\n",
    "    print('No GPU detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2220a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full list of all schizophrenia IDs from csv, 86 in total but not all satisfy have t>90\n",
    "\n",
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0daa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wgan_generator(z_dim):\n",
    "    # Noise input\n",
    "    z_input = Input(shape=(z_dim,))\n",
    "\n",
    "    # Generator network\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Start with a Dense layer to an initial shape that's smaller than the final target\n",
    "    model.add(Dense(128 * 7 * 7 * 9, input_dim=z_dim))  # Adjust to match an initial volume\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Reshape((7, 7, 9, 128)))  # This is the initial volume\n",
    "    \n",
    "    # Begin upsampling to the desired size\n",
    "    model.add(Conv3DTranspose(64, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    # Continue upsampling\n",
    "    model.add(Conv3DTranspose(32, kernel_size=3, strides=(3, 3, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    # Final upsampling step to reach just under the target size\n",
    "    model.add(Conv3DTranspose(1, kernel_size=3, strides=(2, 2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Output tensor\n",
    "    output = model(z_input)\n",
    "\n",
    "    return Model(z_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222b1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wgan_critic(img_shape):\n",
    "    # Image input\n",
    "    img_input = Input(shape=img_shape)\n",
    "\n",
    "    # Critic network\n",
    "    x = Conv3D(64, kernel_size=3, strides=2, padding='same')(img_input)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "\n",
    "    x = Conv3D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    # Output a score for realness (no sigmoid activation)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    return Model(img_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97746e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(real_images, fake_images, critic):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "    # Alpha for interpolation - shape: (batch_size, 1, 1, 1, 1)\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1, 1], 0., 1.)\n",
    "\n",
    "    # Interpolated images - shape: (batch_size, 84, 84, 72, 1)\n",
    "    interpolated_images = (real_images * alpha) + (fake_images * (1 - alpha))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        # Critic now only takes the images as input\n",
    "        predictions = critic(interpolated_images, training=True)\n",
    "\n",
    "    # Calculate the gradients with respect to the interpolated images\n",
    "    gradients = tape.gradient(predictions, [interpolated_images])[0]\n",
    "\n",
    "    # Compute the norm of the gradients - reduce over all dimensions except the batch dimension\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3, 4]))\n",
    "\n",
    "    # Penalize the gradient norm deviation from 1\n",
    "    gp = tf.reduce_mean((gradients_norm - 1.) ** 2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d228f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0]/image.shape[0],\n",
    "        new_shape[1]/image.shape[1],\n",
    "        new_shape[2]/image.shape[2]\n",
    "    )\n",
    "    return scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5858de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan_gp(generator, critic, dataset, epochs, z_dim, lambda_gp, critic_optimizer, generator_optimizer):\n",
    "    # Lists to keep track of losses\n",
    "    critic_losses = []\n",
    "    generator_losses = []\n",
    "\n",
    "    # Directory for saving checkpoints\n",
    "    checkpoint_dir = \"wgan_gp_checkpoints\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_critic_loss = 0.0\n",
    "        epoch_generator_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for real_imgs in dataset:\n",
    "            \n",
    "            num_batches += 1\n",
    "            # Ensure real_imgs converts to numpy array\n",
    "            real_imgs_numpy = real_imgs.numpy() if isinstance(real_imgs, tf.Tensor) else real_imgs\n",
    "\n",
    "            batch_size = real_imgs_numpy.shape[0]\n",
    "            print(real_imgs.shape)\n",
    "            \n",
    "            # Resize real images to match the expected dimensions of the critic\n",
    "            real_imgs_resized = np.array([resize_image(img, (84, 84, 72)) for img in real_imgs.numpy()])\n",
    "            real_imgs_resized = np.expand_dims(real_imgs_resized, axis=-1)  # Add channel dimension\n",
    "\n",
    "            # Train the critic\n",
    "            for _ in range(5):  # Critic is often trained more frequently\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate fake images\n",
    "                    z = tf.random.normal([batch_size, z_dim])\n",
    "                    fake_imgs = generator(z, training=True)\n",
    "\n",
    "                    # Get critic scores for real and fake images\n",
    "                    real_output = critic(real_imgs_resized, training=True)\n",
    "                    fake_output = critic(fake_imgs, training=True)\n",
    "\n",
    "                    # Calculate critic loss\n",
    "                    critic_cost = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "                    gp = gradient_penalty(real_imgs_resized, fake_imgs, critic)\n",
    "                    critic_loss = critic_cost + lambda_gp * gp\n",
    "\n",
    "                # Update critic weights\n",
    "                critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "                critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "                epoch_critic_loss += critic_loss\n",
    "\n",
    "            # Train the generator\n",
    "            z = tf.random.normal([batch_size, z_dim])\n",
    "            \n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_imgs = generator(z, training=True)\n",
    "                fake_output = critic(fake_imgs, training=True)\n",
    "                generator_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "            # Update generator weights\n",
    "            generator_grads = tape.gradient(generator_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "            epoch_generator_loss += generator_loss\n",
    "\n",
    "            # print the losses for each batch\n",
    "            print(f'Epoch: {epoch}, Batch: {num_batches}, Critic Loss: {critic_loss}, Generator Loss: {generator_loss}')\n",
    "\n",
    "        # Checkpointing every n epochs\n",
    "        if (epoch + 1) % 2000 == 0:\n",
    "            generator.save_weights(os.path.join(checkpoint_dir, f\"generator_epoch_{epoch+1}.h5\"))\n",
    "            critic.save_weights(os.path.join(checkpoint_dir, f\"critic_epoch_{epoch+1}.h5\"))\n",
    "            print(f\"Checkpoint: Saved model weights at epoch {epoch+1}\")\n",
    "            \n",
    "        # Record the average losses for this epoch\n",
    "        critic_losses.append(epoch_critic_loss / num_batches)\n",
    "        generator_losses.append(epoch_generator_loss / num_batches)\n",
    "\n",
    "    return critic_losses, generator_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc05a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(d_losses, g_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266d0e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GAN training files processed: 50\n",
      "Total classifier training/testing files processed: 118\n",
      "Total labels processed: 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 03:56:43.927396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46639 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 91, 91, 146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 03:56:45.343297: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-31 03:56:47.914546: I external/local_xla/xla/service/service.cc:168] XLA service 0x97d8050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-31 03:56:47.914565: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-01-31 03:56:47.917711: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706673408.006489 1159338 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fd4088c4680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fd4088c4680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch: 0, Batch: 1, Critic Loss: -18.92547607421875, Generator Loss: -0.3583594858646393\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 0, Batch: 2, Critic Loss: -114.87374877929688, Generator Loss: -0.6727041006088257\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 0, Batch: 3, Critic Loss: -235.33981323242188, Generator Loss: -1.714092493057251\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 0, Batch: 4, Critic Loss: -282.0218505859375, Generator Loss: -2.344866991043091\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 0, Batch: 5, Critic Loss: -225.6396026611328, Generator Loss: -1.8224023580551147\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 1, Batch: 1, Critic Loss: -233.25909423828125, Generator Loss: -1.5804357528686523\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 1, Batch: 2, Critic Loss: -254.89718627929688, Generator Loss: -3.431196689605713\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 1, Batch: 3, Critic Loss: -554.3236083984375, Generator Loss: 1.731987714767456\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 1, Batch: 4, Critic Loss: -267.4429931640625, Generator Loss: 12.925104141235352\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 1, Batch: 5, Critic Loss: -211.73455810546875, Generator Loss: 8.330482482910156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 2, Batch: 1, Critic Loss: -313.279541015625, Generator Loss: 7.564279079437256\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 2, Batch: 2, Critic Loss: -464.36810302734375, Generator Loss: 3.915902614593506\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 2, Batch: 3, Critic Loss: -220.89474487304688, Generator Loss: 4.524657726287842\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 2, Batch: 4, Critic Loss: -279.6297607421875, Generator Loss: 3.982300281524658\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 2, Batch: 5, Critic Loss: -287.8109130859375, Generator Loss: 1.5377905368804932\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 3, Batch: 1, Critic Loss: -290.00628662109375, Generator Loss: -2.029634952545166\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 3, Batch: 2, Critic Loss: -217.8463592529297, Generator Loss: -0.36961764097213745\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 3, Batch: 3, Critic Loss: -305.03607177734375, Generator Loss: -2.6193203926086426\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 3, Batch: 4, Critic Loss: -521.4866943359375, Generator Loss: -15.250802993774414\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 3, Batch: 5, Critic Loss: -222.52273559570312, Generator Loss: -5.796139717102051\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 4, Batch: 1, Critic Loss: -368.0904235839844, Generator Loss: -13.827160835266113\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 4, Batch: 2, Critic Loss: -293.46978759765625, Generator Loss: -13.334393501281738\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 4, Batch: 3, Critic Loss: -250.6405029296875, Generator Loss: -14.73229694366455\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 4, Batch: 4, Critic Loss: -372.3511962890625, Generator Loss: -23.395187377929688\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 4, Batch: 5, Critic Loss: -255.3916778564453, Generator Loss: -22.033748626708984\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 5, Batch: 1, Critic Loss: -472.13848876953125, Generator Loss: -36.63373565673828\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 5, Batch: 2, Critic Loss: -253.6460723876953, Generator Loss: -22.731220245361328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 5, Batch: 3, Critic Loss: -241.8449249267578, Generator Loss: -20.171985626220703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 5, Batch: 4, Critic Loss: -328.3439025878906, Generator Loss: -33.800025939941406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 5, Batch: 5, Critic Loss: -266.4866638183594, Generator Loss: -31.542011260986328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 6, Batch: 1, Critic Loss: -280.8619384765625, Generator Loss: -29.41042709350586\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 6, Batch: 2, Critic Loss: -237.1331787109375, Generator Loss: -29.6018009185791\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 6, Batch: 3, Critic Loss: -262.89373779296875, Generator Loss: -41.0004768371582\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 6, Batch: 4, Critic Loss: -277.8968505859375, Generator Loss: -40.63210678100586\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 6, Batch: 5, Critic Loss: -395.86871337890625, Generator Loss: -48.86198043823242\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 7, Batch: 1, Critic Loss: -321.83624267578125, Generator Loss: -46.479637145996094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 7, Batch: 2, Critic Loss: -186.75474548339844, Generator Loss: -22.232797622680664\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 7, Batch: 3, Critic Loss: -240.80157470703125, Generator Loss: -31.052249908447266\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 7, Batch: 4, Critic Loss: -477.3700866699219, Generator Loss: -69.02665710449219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 7, Batch: 5, Critic Loss: -224.4201202392578, Generator Loss: -35.9541130065918\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 8, Batch: 1, Critic Loss: -206.23483276367188, Generator Loss: -40.67583465576172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 8, Batch: 2, Critic Loss: -359.016357421875, Generator Loss: -45.84585189819336\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 8, Batch: 3, Critic Loss: -295.27685546875, Generator Loss: -63.39417266845703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 8, Batch: 4, Critic Loss: -236.83673095703125, Generator Loss: -40.38815689086914\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 8, Batch: 5, Critic Loss: -397.6101379394531, Generator Loss: -85.3975830078125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 9, Batch: 1, Critic Loss: -152.59593200683594, Generator Loss: -19.683263778686523\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 9, Batch: 2, Critic Loss: -345.70318603515625, Generator Loss: -51.18053436279297\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 9, Batch: 3, Critic Loss: -307.88775634765625, Generator Loss: -70.38037872314453\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 9, Batch: 4, Critic Loss: -319.39599609375, Generator Loss: -82.0383529663086\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 9, Batch: 5, Critic Loss: -273.18255615234375, Generator Loss: -74.60203552246094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 10, Batch: 1, Critic Loss: -343.54632568359375, Generator Loss: -61.99055099487305\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 10, Batch: 2, Critic Loss: -295.88958740234375, Generator Loss: -57.824974060058594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 10, Batch: 3, Critic Loss: -263.36944580078125, Generator Loss: -58.39027786254883\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 10, Batch: 4, Critic Loss: -299.6889343261719, Generator Loss: -73.78934478759766\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 10, Batch: 5, Critic Loss: -186.46310424804688, Generator Loss: -37.60100555419922\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 11, Batch: 1, Critic Loss: -280.7379455566406, Generator Loss: -73.64405822753906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 11, Batch: 2, Critic Loss: -228.02484130859375, Generator Loss: -45.24383544921875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 11, Batch: 3, Critic Loss: -247.4268798828125, Generator Loss: -26.54802894592285\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 11, Batch: 4, Critic Loss: -400.0678405761719, Generator Loss: -158.18910217285156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 11, Batch: 5, Critic Loss: -266.93621826171875, Generator Loss: -73.82616424560547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 12, Batch: 1, Critic Loss: -188.333740234375, Generator Loss: -35.775360107421875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 12, Batch: 2, Critic Loss: -254.89434814453125, Generator Loss: -105.17120361328125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 12, Batch: 3, Critic Loss: -511.1242370605469, Generator Loss: -75.52000427246094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 12, Batch: 4, Critic Loss: -221.36849975585938, Generator Loss: -109.41607666015625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 12, Batch: 5, Critic Loss: -432.1019592285156, Generator Loss: -184.86758422851562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 13, Batch: 1, Critic Loss: -229.20704650878906, Generator Loss: -43.416770935058594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 13, Batch: 2, Critic Loss: -239.4454345703125, Generator Loss: -112.8949203491211\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 13, Batch: 3, Critic Loss: -502.2225341796875, Generator Loss: -161.0073699951172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 13, Batch: 4, Critic Loss: -238.37228393554688, Generator Loss: -67.13207244873047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 13, Batch: 5, Critic Loss: -191.59402465820312, Generator Loss: -108.26153564453125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 14, Batch: 1, Critic Loss: -227.41046142578125, Generator Loss: -86.92345428466797\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 14, Batch: 2, Critic Loss: -329.98162841796875, Generator Loss: -111.50144958496094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 14, Batch: 3, Critic Loss: -253.35836791992188, Generator Loss: -147.3072509765625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 14, Batch: 4, Critic Loss: -349.8636474609375, Generator Loss: -51.47404861450195\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 14, Batch: 5, Critic Loss: -338.33074951171875, Generator Loss: -145.49708557128906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 15, Batch: 1, Critic Loss: -312.41448974609375, Generator Loss: -100.436279296875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 15, Batch: 2, Critic Loss: -223.70986938476562, Generator Loss: -148.57867431640625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 15, Batch: 3, Critic Loss: -210.13815307617188, Generator Loss: -107.30329895019531\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 15, Batch: 4, Critic Loss: -314.2784423828125, Generator Loss: -143.1040496826172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 15, Batch: 5, Critic Loss: -283.8992004394531, Generator Loss: -120.62101745605469\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 16, Batch: 1, Critic Loss: -290.930419921875, Generator Loss: -158.1082305908203\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 16, Batch: 2, Critic Loss: -346.98358154296875, Generator Loss: -68.1293716430664\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 16, Batch: 3, Critic Loss: -243.33099365234375, Generator Loss: -101.58721923828125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 16, Batch: 4, Critic Loss: -333.90289306640625, Generator Loss: -161.0302276611328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 16, Batch: 5, Critic Loss: -233.28201293945312, Generator Loss: -115.03105163574219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 17, Batch: 1, Critic Loss: -242.14236450195312, Generator Loss: -120.301513671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 17, Batch: 2, Critic Loss: -248.49423217773438, Generator Loss: -105.9449462890625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 17, Batch: 3, Critic Loss: -223.5691680908203, Generator Loss: -147.31597900390625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 17, Batch: 4, Critic Loss: -235.17625427246094, Generator Loss: -78.98651123046875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 17, Batch: 5, Critic Loss: -326.1967468261719, Generator Loss: -106.8644027709961\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 18, Batch: 1, Critic Loss: -291.99072265625, Generator Loss: -97.85176086425781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 18, Batch: 2, Critic Loss: -267.0363464355469, Generator Loss: -158.84414672851562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 18, Batch: 3, Critic Loss: -312.34423828125, Generator Loss: -171.27651977539062\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 18, Batch: 4, Critic Loss: -245.66665649414062, Generator Loss: -125.213623046875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 18, Batch: 5, Critic Loss: -260.98968505859375, Generator Loss: -120.60182189941406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 19, Batch: 1, Critic Loss: -225.14120483398438, Generator Loss: -178.15354919433594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 19, Batch: 2, Critic Loss: -380.4874267578125, Generator Loss: -135.38870239257812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 19, Batch: 3, Critic Loss: -306.0971374511719, Generator Loss: -72.89530944824219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 19, Batch: 4, Critic Loss: -324.1750183105469, Generator Loss: -214.21768188476562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 19, Batch: 5, Critic Loss: -207.62290954589844, Generator Loss: -76.78714752197266\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 20, Batch: 1, Critic Loss: -244.00338745117188, Generator Loss: -195.07028198242188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 20, Batch: 2, Critic Loss: -384.1645202636719, Generator Loss: -154.97764587402344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 20, Batch: 3, Critic Loss: -252.01922607421875, Generator Loss: -168.85198974609375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 20, Batch: 4, Critic Loss: -302.226318359375, Generator Loss: -183.78475952148438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 20, Batch: 5, Critic Loss: -148.44931030273438, Generator Loss: -77.59855651855469\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 21, Batch: 1, Critic Loss: -227.4215850830078, Generator Loss: -169.52650451660156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 21, Batch: 2, Critic Loss: -195.98223876953125, Generator Loss: -134.15243530273438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 21, Batch: 3, Critic Loss: -160.0147705078125, Generator Loss: -121.0750961303711\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 21, Batch: 4, Critic Loss: -364.8966979980469, Generator Loss: -212.3016815185547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 21, Batch: 5, Critic Loss: -239.1533966064453, Generator Loss: -198.0609588623047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 22, Batch: 1, Critic Loss: -217.52899169921875, Generator Loss: -197.30258178710938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 22, Batch: 2, Critic Loss: -275.81768798828125, Generator Loss: -175.52972412109375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 22, Batch: 3, Critic Loss: -159.00076293945312, Generator Loss: -128.45521545410156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 22, Batch: 4, Critic Loss: -258.1968994140625, Generator Loss: -173.15118408203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 22, Batch: 5, Critic Loss: -248.88421630859375, Generator Loss: -231.1151885986328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 23, Batch: 1, Critic Loss: -124.2352294921875, Generator Loss: -90.4571533203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 23, Batch: 2, Critic Loss: -198.3023223876953, Generator Loss: -136.55178833007812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 23, Batch: 3, Critic Loss: -268.9367370605469, Generator Loss: -156.78042602539062\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 23, Batch: 4, Critic Loss: -349.82958984375, Generator Loss: -180.3866729736328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 23, Batch: 5, Critic Loss: -257.60272216796875, Generator Loss: -132.30426025390625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 24, Batch: 1, Critic Loss: -310.2244873046875, Generator Loss: -143.2625732421875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 24, Batch: 2, Critic Loss: -394.5268859863281, Generator Loss: -233.56826782226562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 24, Batch: 3, Critic Loss: -217.7896728515625, Generator Loss: -181.0384979248047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 24, Batch: 4, Critic Loss: -107.22329711914062, Generator Loss: -92.05279541015625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 24, Batch: 5, Critic Loss: -198.5287322998047, Generator Loss: -154.6167755126953\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 25, Batch: 1, Critic Loss: -216.0028076171875, Generator Loss: -126.90348815917969\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 25, Batch: 2, Critic Loss: -178.9507598876953, Generator Loss: -151.53091430664062\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 25, Batch: 3, Critic Loss: -300.71490478515625, Generator Loss: -128.66055297851562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 25, Batch: 4, Critic Loss: -274.45831298828125, Generator Loss: -163.105224609375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 25, Batch: 5, Critic Loss: -224.33657836914062, Generator Loss: -122.1939468383789\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 26, Batch: 1, Critic Loss: -110.43330383300781, Generator Loss: -117.49491882324219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 26, Batch: 2, Critic Loss: -226.7362060546875, Generator Loss: -197.74591064453125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 26, Batch: 3, Critic Loss: -428.0523681640625, Generator Loss: -283.6372375488281\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 26, Batch: 4, Critic Loss: -272.1717834472656, Generator Loss: -203.9810028076172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 26, Batch: 5, Critic Loss: -164.34263610839844, Generator Loss: -73.24916076660156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 27, Batch: 1, Critic Loss: -167.31932067871094, Generator Loss: -104.54679107666016\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 27, Batch: 2, Critic Loss: -146.85748291015625, Generator Loss: -106.6673812866211\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 27, Batch: 3, Critic Loss: -176.86598205566406, Generator Loss: -147.10397338867188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 27, Batch: 4, Critic Loss: -283.9087829589844, Generator Loss: -183.08248901367188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 27, Batch: 5, Critic Loss: -218.30096435546875, Generator Loss: -111.89164733886719\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 28, Batch: 1, Critic Loss: -226.39324951171875, Generator Loss: -122.67999267578125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 28, Batch: 2, Critic Loss: -276.32647705078125, Generator Loss: -312.2400207519531\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 28, Batch: 3, Critic Loss: -192.80738830566406, Generator Loss: -126.28318786621094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 28, Batch: 4, Critic Loss: -153.3727264404297, Generator Loss: -118.779541015625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 28, Batch: 5, Critic Loss: -246.10781860351562, Generator Loss: -227.3319854736328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 29, Batch: 1, Critic Loss: -213.57699584960938, Generator Loss: -218.57571411132812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 29, Batch: 2, Critic Loss: -155.804443359375, Generator Loss: -118.38739013671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 29, Batch: 3, Critic Loss: -186.03863525390625, Generator Loss: -183.0868682861328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 29, Batch: 4, Critic Loss: -403.357177734375, Generator Loss: -327.4175720214844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 29, Batch: 5, Critic Loss: -90.83499145507812, Generator Loss: -98.86808776855469\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 30, Batch: 1, Critic Loss: -205.26353454589844, Generator Loss: -162.22250366210938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 30, Batch: 2, Critic Loss: -259.4110412597656, Generator Loss: -177.28021240234375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 30, Batch: 3, Critic Loss: -150.39854431152344, Generator Loss: -163.8267364501953\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 30, Batch: 4, Critic Loss: -165.4756317138672, Generator Loss: -121.24678802490234\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 30, Batch: 5, Critic Loss: -204.12255859375, Generator Loss: -113.64210510253906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 31, Batch: 1, Critic Loss: -137.5118865966797, Generator Loss: -96.05809020996094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 31, Batch: 2, Critic Loss: -199.15133666992188, Generator Loss: -145.33653259277344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 31, Batch: 3, Critic Loss: -139.22549438476562, Generator Loss: -47.643516540527344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 31, Batch: 4, Critic Loss: -332.1020202636719, Generator Loss: -296.9933166503906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 31, Batch: 5, Critic Loss: -198.52947998046875, Generator Loss: -255.1381378173828\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 32, Batch: 1, Critic Loss: -219.05569458007812, Generator Loss: -189.3036346435547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 32, Batch: 2, Critic Loss: -127.15914916992188, Generator Loss: -166.20596313476562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 32, Batch: 3, Critic Loss: -144.86233520507812, Generator Loss: -144.3870391845703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 32, Batch: 4, Critic Loss: -178.13656616210938, Generator Loss: -158.5651092529297\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 32, Batch: 5, Critic Loss: -252.16815185546875, Generator Loss: -208.1125946044922\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 33, Batch: 1, Critic Loss: -139.65988159179688, Generator Loss: -170.34481811523438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 33, Batch: 2, Critic Loss: -406.53338623046875, Generator Loss: -139.64779663085938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 33, Batch: 3, Critic Loss: -144.13905334472656, Generator Loss: -167.32672119140625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 33, Batch: 4, Critic Loss: -161.96197509765625, Generator Loss: -203.92587280273438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 33, Batch: 5, Critic Loss: -98.57373046875, Generator Loss: -166.87257385253906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 34, Batch: 1, Critic Loss: -91.06293487548828, Generator Loss: -148.6385040283203\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 34, Batch: 2, Critic Loss: -316.83868408203125, Generator Loss: -255.80484008789062\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 34, Batch: 3, Critic Loss: -178.66934204101562, Generator Loss: -155.83152770996094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 34, Batch: 4, Critic Loss: -189.05853271484375, Generator Loss: -92.79405212402344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 34, Batch: 5, Critic Loss: -187.06344604492188, Generator Loss: -184.71231079101562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 35, Batch: 1, Critic Loss: -255.925048828125, Generator Loss: -318.690185546875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 35, Batch: 2, Critic Loss: -170.85494995117188, Generator Loss: -167.8126983642578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 35, Batch: 3, Critic Loss: -100.71980285644531, Generator Loss: -95.38398742675781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 35, Batch: 4, Critic Loss: -145.19479370117188, Generator Loss: -162.04837036132812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 35, Batch: 5, Critic Loss: -229.64730834960938, Generator Loss: -178.75965881347656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 36, Batch: 1, Critic Loss: -170.58535766601562, Generator Loss: -177.4144744873047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 36, Batch: 2, Critic Loss: -109.24356079101562, Generator Loss: -130.2842254638672\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 36, Batch: 3, Critic Loss: -136.73532104492188, Generator Loss: -106.95391845703125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 36, Batch: 4, Critic Loss: -211.97744750976562, Generator Loss: -202.84732055664062\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 36, Batch: 5, Critic Loss: -305.1962890625, Generator Loss: -153.8494110107422\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 37, Batch: 1, Critic Loss: -106.89727783203125, Generator Loss: -124.0545883178711\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 37, Batch: 2, Critic Loss: -230.642822265625, Generator Loss: -165.65573120117188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 37, Batch: 3, Critic Loss: -161.22129821777344, Generator Loss: -79.89183807373047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 37, Batch: 4, Critic Loss: -216.79666137695312, Generator Loss: -199.5616455078125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 37, Batch: 5, Critic Loss: -75.14369201660156, Generator Loss: -74.36128234863281\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 38, Batch: 1, Critic Loss: -198.1044921875, Generator Loss: -201.85987854003906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 38, Batch: 2, Critic Loss: -93.92022705078125, Generator Loss: -80.41749572753906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 38, Batch: 3, Critic Loss: -123.10478210449219, Generator Loss: -102.97469329833984\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 38, Batch: 4, Critic Loss: -354.2737731933594, Generator Loss: -202.83895874023438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 38, Batch: 5, Critic Loss: -135.88150024414062, Generator Loss: -133.9910888671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 39, Batch: 1, Critic Loss: -108.34732055664062, Generator Loss: -99.68545532226562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 39, Batch: 2, Critic Loss: -143.5109405517578, Generator Loss: -95.12467193603516\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 39, Batch: 3, Critic Loss: -91.67817687988281, Generator Loss: -124.45501708984375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 39, Batch: 4, Critic Loss: -195.02532958984375, Generator Loss: -219.6818389892578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 39, Batch: 5, Critic Loss: -205.13430786132812, Generator Loss: -280.5877990722656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 40, Batch: 1, Critic Loss: -63.973350524902344, Generator Loss: -137.8372344970703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 40, Batch: 2, Critic Loss: -213.31939697265625, Generator Loss: -134.14736938476562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 40, Batch: 3, Critic Loss: -111.28672790527344, Generator Loss: -171.52401733398438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 40, Batch: 4, Critic Loss: -181.36351013183594, Generator Loss: -123.77815246582031\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 40, Batch: 5, Critic Loss: -168.57901000976562, Generator Loss: -191.72413635253906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 41, Batch: 1, Critic Loss: -231.87741088867188, Generator Loss: -167.98385620117188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 41, Batch: 2, Critic Loss: -169.0423126220703, Generator Loss: -208.8010711669922\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 41, Batch: 3, Critic Loss: -146.22280883789062, Generator Loss: -135.848388671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 41, Batch: 4, Critic Loss: -212.919677734375, Generator Loss: -134.9326934814453\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 41, Batch: 5, Critic Loss: -56.90422821044922, Generator Loss: -61.683509826660156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 42, Batch: 1, Critic Loss: -279.41851806640625, Generator Loss: -165.1742706298828\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 42, Batch: 2, Critic Loss: -61.793052673339844, Generator Loss: -78.25914001464844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 42, Batch: 3, Critic Loss: -174.60226440429688, Generator Loss: -96.34913635253906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 42, Batch: 4, Critic Loss: -83.75578308105469, Generator Loss: -127.64271545410156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 42, Batch: 5, Critic Loss: -127.5257568359375, Generator Loss: -91.90925598144531\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 43, Batch: 1, Critic Loss: -149.75448608398438, Generator Loss: -102.70865631103516\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 43, Batch: 2, Critic Loss: -75.93052673339844, Generator Loss: -143.63851928710938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 43, Batch: 3, Critic Loss: -133.82533264160156, Generator Loss: -113.43516540527344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 43, Batch: 4, Critic Loss: -207.10093688964844, Generator Loss: -131.58236694335938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 43, Batch: 5, Critic Loss: -174.72781372070312, Generator Loss: -146.05758666992188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 44, Batch: 1, Critic Loss: -192.04220581054688, Generator Loss: -194.0122528076172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 44, Batch: 2, Critic Loss: -133.1317901611328, Generator Loss: -109.4431381225586\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 44, Batch: 3, Critic Loss: -222.5519256591797, Generator Loss: -130.501220703125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 44, Batch: 4, Critic Loss: -116.47944641113281, Generator Loss: -92.33226013183594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 44, Batch: 5, Critic Loss: -102.84513854980469, Generator Loss: -78.92588806152344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 45, Batch: 1, Critic Loss: -146.40716552734375, Generator Loss: -107.71308898925781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 45, Batch: 2, Critic Loss: -119.68971252441406, Generator Loss: -157.65977478027344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 45, Batch: 3, Critic Loss: -188.61941528320312, Generator Loss: -191.03526306152344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 45, Batch: 4, Critic Loss: -104.71493530273438, Generator Loss: -67.42842864990234\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 45, Batch: 5, Critic Loss: -205.61915588378906, Generator Loss: -109.67462158203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 46, Batch: 1, Critic Loss: -108.1678237915039, Generator Loss: -116.65682220458984\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 46, Batch: 2, Critic Loss: -124.99078369140625, Generator Loss: -101.33363342285156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 46, Batch: 3, Critic Loss: -153.91748046875, Generator Loss: -115.3834457397461\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 46, Batch: 4, Critic Loss: -127.469482421875, Generator Loss: -123.43464660644531\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 46, Batch: 5, Critic Loss: -68.71410369873047, Generator Loss: -80.90400695800781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 47, Batch: 1, Critic Loss: -134.56532287597656, Generator Loss: -87.96073150634766\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 47, Batch: 2, Critic Loss: -144.18527221679688, Generator Loss: -66.11589813232422\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 47, Batch: 3, Critic Loss: -76.33615112304688, Generator Loss: -21.51818084716797\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 47, Batch: 4, Critic Loss: -127.39984130859375, Generator Loss: -117.49446868896484\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 47, Batch: 5, Critic Loss: -111.49613952636719, Generator Loss: -145.415283203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 48, Batch: 1, Critic Loss: -180.94998168945312, Generator Loss: -123.57408142089844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 48, Batch: 2, Critic Loss: -139.81756591796875, Generator Loss: -102.0545654296875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 48, Batch: 3, Critic Loss: -92.50018310546875, Generator Loss: -86.36781311035156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 48, Batch: 4, Critic Loss: -185.36224365234375, Generator Loss: -154.6779327392578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 48, Batch: 5, Critic Loss: -86.21786499023438, Generator Loss: -89.3291244506836\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 49, Batch: 1, Critic Loss: -210.885986328125, Generator Loss: -128.79652404785156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 49, Batch: 2, Critic Loss: -132.83885192871094, Generator Loss: -74.90714263916016\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 49, Batch: 3, Critic Loss: -185.3520050048828, Generator Loss: -139.64430236816406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 49, Batch: 4, Critic Loss: -73.70380401611328, Generator Loss: -36.20507049560547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 49, Batch: 5, Critic Loss: -124.55352020263672, Generator Loss: -56.75518798828125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 50, Batch: 1, Critic Loss: -95.8431167602539, Generator Loss: -66.78544616699219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 50, Batch: 2, Critic Loss: -206.47665405273438, Generator Loss: -197.89955139160156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 50, Batch: 3, Critic Loss: -109.75562286376953, Generator Loss: -114.04771423339844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 50, Batch: 4, Critic Loss: -141.0155029296875, Generator Loss: -88.03314208984375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 50, Batch: 5, Critic Loss: -121.05655670166016, Generator Loss: -66.9337158203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 51, Batch: 1, Critic Loss: -137.51126098632812, Generator Loss: -123.53471374511719\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 51, Batch: 2, Critic Loss: -82.49530792236328, Generator Loss: -61.7422981262207\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 51, Batch: 3, Critic Loss: -146.1268310546875, Generator Loss: -76.84599304199219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 51, Batch: 4, Critic Loss: -154.98025512695312, Generator Loss: -128.84365844726562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 51, Batch: 5, Critic Loss: -127.95940399169922, Generator Loss: -75.70536804199219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 52, Batch: 1, Critic Loss: -114.09988403320312, Generator Loss: -109.09375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 52, Batch: 2, Critic Loss: -72.4598159790039, Generator Loss: -68.96257019042969\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 52, Batch: 3, Critic Loss: -102.60220336914062, Generator Loss: -69.16505432128906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 52, Batch: 4, Critic Loss: -237.37039184570312, Generator Loss: -56.44092559814453\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 52, Batch: 5, Critic Loss: -136.7190399169922, Generator Loss: -105.88505554199219\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 53, Batch: 1, Critic Loss: -88.77127075195312, Generator Loss: -76.35224151611328\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 53, Batch: 2, Critic Loss: -78.54994201660156, Generator Loss: -75.70267486572266\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 53, Batch: 3, Critic Loss: -154.49758911132812, Generator Loss: -154.5676727294922\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 53, Batch: 4, Critic Loss: -155.43728637695312, Generator Loss: -156.48690795898438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 53, Batch: 5, Critic Loss: -165.71054077148438, Generator Loss: -150.71949768066406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 54, Batch: 1, Critic Loss: -134.12322998046875, Generator Loss: -35.39707565307617\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 54, Batch: 2, Critic Loss: -107.4349136352539, Generator Loss: -91.33381652832031\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 54, Batch: 3, Critic Loss: -79.40513610839844, Generator Loss: -42.99553298950195\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 54, Batch: 4, Critic Loss: -112.31987762451172, Generator Loss: -64.7763900756836\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 54, Batch: 5, Critic Loss: -174.51617431640625, Generator Loss: -152.92715454101562\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 55, Batch: 1, Critic Loss: -153.90451049804688, Generator Loss: -108.05381774902344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 55, Batch: 2, Critic Loss: -137.90869140625, Generator Loss: -105.9892578125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 55, Batch: 3, Critic Loss: -199.72409057617188, Generator Loss: -132.1129913330078\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 55, Batch: 4, Critic Loss: -134.7548828125, Generator Loss: -143.06982421875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 55, Batch: 5, Critic Loss: -64.97711181640625, Generator Loss: -63.18723678588867\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 56, Batch: 1, Critic Loss: -120.86083984375, Generator Loss: -38.4029655456543\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 56, Batch: 2, Critic Loss: -106.36207580566406, Generator Loss: -57.9023551940918\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 56, Batch: 3, Critic Loss: -97.50311279296875, Generator Loss: -73.09645080566406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 56, Batch: 4, Critic Loss: -172.20913696289062, Generator Loss: -91.60101318359375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 56, Batch: 5, Critic Loss: -96.95704650878906, Generator Loss: -191.5814666748047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 57, Batch: 1, Critic Loss: -232.8196258544922, Generator Loss: -153.61106872558594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 57, Batch: 2, Critic Loss: -122.15660095214844, Generator Loss: -112.14143371582031\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 57, Batch: 3, Critic Loss: -110.55816650390625, Generator Loss: -50.049644470214844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 57, Batch: 4, Critic Loss: -182.25100708007812, Generator Loss: -59.71398162841797\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 57, Batch: 5, Critic Loss: -87.68376922607422, Generator Loss: -49.91535186767578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 58, Batch: 1, Critic Loss: -158.45457458496094, Generator Loss: -21.90106773376465\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 58, Batch: 2, Critic Loss: -103.47148895263672, Generator Loss: -54.100616455078125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 58, Batch: 3, Critic Loss: -110.69424438476562, Generator Loss: -10.205327033996582\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 58, Batch: 4, Critic Loss: -120.17334747314453, Generator Loss: -55.30466842651367\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 58, Batch: 5, Critic Loss: -121.0338134765625, Generator Loss: -45.628055572509766\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 59, Batch: 1, Critic Loss: -142.0408935546875, Generator Loss: 5.456750392913818\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 59, Batch: 2, Critic Loss: -113.2491226196289, Generator Loss: 6.770195960998535\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 59, Batch: 3, Critic Loss: -169.71298217773438, Generator Loss: -143.99232482910156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 59, Batch: 4, Critic Loss: -121.17575073242188, Generator Loss: -79.44493103027344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 59, Batch: 5, Critic Loss: -121.2309341430664, Generator Loss: -100.94532775878906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 60, Batch: 1, Critic Loss: -271.75018310546875, Generator Loss: -217.727294921875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 60, Batch: 2, Critic Loss: -69.64802551269531, Generator Loss: -149.92465209960938\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 60, Batch: 3, Critic Loss: -104.00650024414062, Generator Loss: -14.937838554382324\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 60, Batch: 4, Critic Loss: -154.90084838867188, Generator Loss: 3.7558188438415527\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 60, Batch: 5, Critic Loss: -97.05497741699219, Generator Loss: -51.792144775390625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 61, Batch: 1, Critic Loss: -226.75997924804688, Generator Loss: -139.85560607910156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 61, Batch: 2, Critic Loss: -93.61258697509766, Generator Loss: 9.548230171203613\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 61, Batch: 3, Critic Loss: -109.18096160888672, Generator Loss: -49.0871696472168\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 61, Batch: 4, Critic Loss: -126.35969543457031, Generator Loss: -47.89257049560547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 61, Batch: 5, Critic Loss: -93.74986267089844, Generator Loss: -24.976146697998047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 62, Batch: 1, Critic Loss: -182.7108917236328, Generator Loss: -17.169342041015625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 62, Batch: 2, Critic Loss: -96.08685302734375, Generator Loss: -52.2476921081543\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 62, Batch: 3, Critic Loss: -125.97453308105469, Generator Loss: -20.9682559967041\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 62, Batch: 4, Critic Loss: -139.96189880371094, Generator Loss: -141.8783416748047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 62, Batch: 5, Critic Loss: -272.136962890625, Generator Loss: -255.74172973632812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 63, Batch: 1, Critic Loss: -133.70584106445312, Generator Loss: -170.30075073242188\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 63, Batch: 2, Critic Loss: -103.43193817138672, Generator Loss: 33.17119216918945\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 63, Batch: 3, Critic Loss: -120.88211822509766, Generator Loss: -1.1849857568740845\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 63, Batch: 4, Critic Loss: -154.2322998046875, Generator Loss: 117.553955078125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 63, Batch: 5, Critic Loss: -142.46206665039062, Generator Loss: -55.791351318359375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 64, Batch: 1, Critic Loss: -240.64382934570312, Generator Loss: 107.32069396972656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 64, Batch: 2, Critic Loss: -104.42338562011719, Generator Loss: -132.555419921875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 64, Batch: 3, Critic Loss: -139.7781982421875, Generator Loss: -158.5899658203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 64, Batch: 4, Critic Loss: -113.16825103759766, Generator Loss: -76.74761962890625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 64, Batch: 5, Critic Loss: -102.2706298828125, Generator Loss: -76.30233001708984\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 65, Batch: 1, Critic Loss: -128.61306762695312, Generator Loss: -85.91563415527344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 65, Batch: 2, Critic Loss: -118.71881103515625, Generator Loss: -81.91835021972656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 65, Batch: 3, Critic Loss: -188.46661376953125, Generator Loss: -100.35759735107422\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 65, Batch: 4, Critic Loss: -104.11124420166016, Generator Loss: -40.8150634765625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 65, Batch: 5, Critic Loss: -106.962890625, Generator Loss: -33.55562210083008\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 66, Batch: 1, Critic Loss: -77.64080047607422, Generator Loss: -76.37893676757812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 66, Batch: 2, Critic Loss: -136.3556365966797, Generator Loss: -47.61164093017578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 66, Batch: 3, Critic Loss: -166.650390625, Generator Loss: 29.25640296936035\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 66, Batch: 4, Critic Loss: -150.1145782470703, Generator Loss: 62.5797233581543\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 66, Batch: 5, Critic Loss: -132.23997497558594, Generator Loss: -150.2764129638672\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 67, Batch: 1, Critic Loss: -63.18281555175781, Generator Loss: -206.8169403076172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 67, Batch: 2, Critic Loss: -106.90245819091797, Generator Loss: -41.07695388793945\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 67, Batch: 3, Critic Loss: -135.5548095703125, Generator Loss: -79.26876068115234\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 67, Batch: 4, Critic Loss: -197.0238037109375, Generator Loss: -33.90264892578125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 67, Batch: 5, Critic Loss: -126.484130859375, Generator Loss: 13.665807723999023\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 68, Batch: 1, Critic Loss: -103.11766815185547, Generator Loss: -68.45610046386719\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 68, Batch: 2, Critic Loss: -131.01864624023438, Generator Loss: -85.7747802734375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 68, Batch: 3, Critic Loss: -136.31243896484375, Generator Loss: -3.3423514366149902\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 68, Batch: 4, Critic Loss: -121.24225616455078, Generator Loss: -45.72431564331055\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 68, Batch: 5, Critic Loss: -167.42129516601562, Generator Loss: -71.5284423828125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 69, Batch: 1, Critic Loss: -107.85304260253906, Generator Loss: -132.38693237304688\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 69, Batch: 2, Critic Loss: -199.08218383789062, Generator Loss: -95.49781799316406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 69, Batch: 3, Critic Loss: -119.72479248046875, Generator Loss: -19.770219802856445\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 69, Batch: 4, Critic Loss: -79.38136291503906, Generator Loss: -17.307239532470703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 69, Batch: 5, Critic Loss: -93.13907623291016, Generator Loss: 41.491050720214844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 70, Batch: 1, Critic Loss: -140.79483032226562, Generator Loss: -59.90375900268555\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 70, Batch: 2, Critic Loss: -108.64628601074219, Generator Loss: -64.63443756103516\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 70, Batch: 3, Critic Loss: -170.75328063964844, Generator Loss: -44.94813537597656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 70, Batch: 4, Critic Loss: -102.97016143798828, Generator Loss: -24.17476463317871\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 70, Batch: 5, Critic Loss: -161.5732421875, Generator Loss: 8.323140144348145\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 71, Batch: 1, Critic Loss: -253.83041381835938, Generator Loss: 26.110849380493164\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 71, Batch: 2, Critic Loss: -105.103759765625, Generator Loss: -177.36932373046875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 71, Batch: 3, Critic Loss: -172.33328247070312, Generator Loss: -88.53584289550781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 71, Batch: 4, Critic Loss: -147.32516479492188, Generator Loss: 10.7311372756958\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 71, Batch: 5, Critic Loss: -104.4405288696289, Generator Loss: -8.742914199829102\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 72, Batch: 1, Critic Loss: -170.5479278564453, Generator Loss: -130.60223388671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 72, Batch: 2, Critic Loss: -116.97587585449219, Generator Loss: -45.95444107055664\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 72, Batch: 3, Critic Loss: -125.19395446777344, Generator Loss: 20.279132843017578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 72, Batch: 4, Critic Loss: -86.69282531738281, Generator Loss: 5.350878715515137\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 72, Batch: 5, Critic Loss: -218.80328369140625, Generator Loss: 14.179420471191406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 73, Batch: 1, Critic Loss: -258.6086120605469, Generator Loss: -37.41736602783203\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 73, Batch: 2, Critic Loss: -110.43367767333984, Generator Loss: 59.618682861328125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 73, Batch: 3, Critic Loss: -66.93568420410156, Generator Loss: -9.160959243774414\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 73, Batch: 4, Critic Loss: -142.8155517578125, Generator Loss: 15.067339897155762\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 73, Batch: 5, Critic Loss: -164.87319946289062, Generator Loss: -28.155025482177734\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 74, Batch: 1, Critic Loss: -173.8621826171875, Generator Loss: -28.114055633544922\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 74, Batch: 2, Critic Loss: -162.5174102783203, Generator Loss: -39.81639862060547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 74, Batch: 3, Critic Loss: -116.08768463134766, Generator Loss: 7.227606296539307\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 74, Batch: 4, Critic Loss: -130.1992645263672, Generator Loss: -43.259803771972656\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 74, Batch: 5, Critic Loss: -130.1957244873047, Generator Loss: -14.931581497192383\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 75, Batch: 1, Critic Loss: -106.5389175415039, Generator Loss: 13.564157485961914\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 75, Batch: 2, Critic Loss: -151.9552764892578, Generator Loss: 6.075840950012207\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 75, Batch: 3, Critic Loss: -152.73728942871094, Generator Loss: -42.87316131591797\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 75, Batch: 4, Critic Loss: -126.55107116699219, Generator Loss: -81.55537414550781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 75, Batch: 5, Critic Loss: -146.25180053710938, Generator Loss: -45.72813034057617\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 76, Batch: 1, Critic Loss: -110.50930786132812, Generator Loss: -14.434431076049805\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 76, Batch: 2, Critic Loss: -201.2572479248047, Generator Loss: -20.974323272705078\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 76, Batch: 3, Critic Loss: -115.10247802734375, Generator Loss: -35.144283294677734\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 76, Batch: 4, Critic Loss: -144.797119140625, Generator Loss: -126.25765228271484\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 76, Batch: 5, Critic Loss: -126.63650512695312, Generator Loss: -57.4093017578125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 77, Batch: 1, Critic Loss: -188.28370666503906, Generator Loss: 40.48411178588867\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 77, Batch: 2, Critic Loss: -176.7509002685547, Generator Loss: 9.219806671142578\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 77, Batch: 3, Critic Loss: -103.2239990234375, Generator Loss: -22.93511962890625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 77, Batch: 4, Critic Loss: -127.81704711914062, Generator Loss: -7.397311210632324\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 77, Batch: 5, Critic Loss: -85.56354522705078, Generator Loss: -42.56061553955078\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 78, Batch: 1, Critic Loss: -146.79483032226562, Generator Loss: -20.024951934814453\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 78, Batch: 2, Critic Loss: -131.17703247070312, Generator Loss: -73.22648620605469\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 78, Batch: 3, Critic Loss: -204.00595092773438, Generator Loss: -50.17984390258789\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 78, Batch: 4, Critic Loss: -133.05125427246094, Generator Loss: -43.70001220703125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 78, Batch: 5, Critic Loss: -233.73513793945312, Generator Loss: 8.485544204711914\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 79, Batch: 1, Critic Loss: -127.5621337890625, Generator Loss: -3.3017311096191406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 79, Batch: 2, Critic Loss: -117.69719696044922, Generator Loss: -76.36787414550781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 79, Batch: 3, Critic Loss: -155.30618286132812, Generator Loss: -69.60322570800781\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 79, Batch: 4, Critic Loss: -87.00576782226562, Generator Loss: -14.632818222045898\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 79, Batch: 5, Critic Loss: -90.58047485351562, Generator Loss: -38.302696228027344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 80, Batch: 1, Critic Loss: -118.663330078125, Generator Loss: -23.443531036376953\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 80, Batch: 2, Critic Loss: -140.6125946044922, Generator Loss: -43.74162673950195\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 80, Batch: 3, Critic Loss: -176.83657836914062, Generator Loss: -150.8340301513672\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 80, Batch: 4, Critic Loss: -82.06109619140625, Generator Loss: 3.139312267303467\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 80, Batch: 5, Critic Loss: -241.43885803222656, Generator Loss: 42.346107482910156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 81, Batch: 1, Critic Loss: -107.11018371582031, Generator Loss: -18.362621307373047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 81, Batch: 2, Critic Loss: -167.32925415039062, Generator Loss: -47.60073471069336\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 81, Batch: 3, Critic Loss: -132.778564453125, Generator Loss: 36.60112762451172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 81, Batch: 4, Critic Loss: -182.3610382080078, Generator Loss: 83.34356689453125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 81, Batch: 5, Critic Loss: -218.90805053710938, Generator Loss: 3.256568193435669\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 82, Batch: 1, Critic Loss: -128.48324584960938, Generator Loss: -60.66424560546875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 82, Batch: 2, Critic Loss: -176.90896606445312, Generator Loss: -35.9727668762207\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 82, Batch: 3, Critic Loss: -134.7010498046875, Generator Loss: 5.266730308532715\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 82, Batch: 4, Critic Loss: -148.02345275878906, Generator Loss: -24.39832878112793\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 82, Batch: 5, Critic Loss: -141.3614044189453, Generator Loss: -40.356876373291016\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 83, Batch: 1, Critic Loss: -129.07131958007812, Generator Loss: 15.271196365356445\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 83, Batch: 2, Critic Loss: -143.59927368164062, Generator Loss: 10.135833740234375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 83, Batch: 3, Critic Loss: -98.15799713134766, Generator Loss: -19.8774471282959\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 83, Batch: 4, Critic Loss: -106.14471435546875, Generator Loss: -158.08798217773438\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 83, Batch: 5, Critic Loss: -205.96226501464844, Generator Loss: -56.08967971801758\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 84, Batch: 1, Critic Loss: -199.3388671875, Generator Loss: -214.78955078125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 84, Batch: 2, Critic Loss: -190.68902587890625, Generator Loss: -183.8348388671875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 84, Batch: 3, Critic Loss: -89.44929504394531, Generator Loss: -18.209957122802734\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 84, Batch: 4, Critic Loss: -113.83124542236328, Generator Loss: 26.301422119140625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 84, Batch: 5, Critic Loss: -250.51675415039062, Generator Loss: 51.03463363647461\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 85, Batch: 1, Critic Loss: -248.29029846191406, Generator Loss: 243.1459197998047\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 85, Batch: 2, Critic Loss: -91.24150848388672, Generator Loss: 153.6836395263672\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 85, Batch: 3, Critic Loss: -178.78756713867188, Generator Loss: 114.05096435546875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 85, Batch: 4, Critic Loss: -109.1340103149414, Generator Loss: 95.19867706298828\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 85, Batch: 5, Critic Loss: -182.48504638671875, Generator Loss: -93.96412658691406\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 86, Batch: 1, Critic Loss: -163.71522521972656, Generator Loss: -174.45196533203125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 86, Batch: 2, Critic Loss: -148.3741912841797, Generator Loss: -58.9865608215332\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 86, Batch: 3, Critic Loss: -122.40176391601562, Generator Loss: -86.67630004882812\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 86, Batch: 4, Critic Loss: -149.55133056640625, Generator Loss: 11.261147499084473\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 86, Batch: 5, Critic Loss: -127.51829528808594, Generator Loss: -62.039024353027344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 87, Batch: 1, Critic Loss: -160.7533721923828, Generator Loss: -99.05915832519531\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 87, Batch: 2, Critic Loss: -157.3040313720703, Generator Loss: 8.312079429626465\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 87, Batch: 3, Critic Loss: -96.09358215332031, Generator Loss: 16.063508987426758\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 87, Batch: 4, Critic Loss: -116.85198974609375, Generator Loss: 25.998098373413086\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 87, Batch: 5, Critic Loss: -161.82611083984375, Generator Loss: 32.555511474609375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 88, Batch: 1, Critic Loss: -147.5404052734375, Generator Loss: -11.876572608947754\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 88, Batch: 2, Critic Loss: -153.80697631835938, Generator Loss: 11.353217124938965\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 88, Batch: 3, Critic Loss: -212.54498291015625, Generator Loss: 27.503692626953125\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 88, Batch: 4, Critic Loss: -149.01524353027344, Generator Loss: 52.03568649291992\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 88, Batch: 5, Critic Loss: -128.4407958984375, Generator Loss: 10.981026649475098\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 89, Batch: 1, Critic Loss: -174.19393920898438, Generator Loss: -85.44119262695312\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 89, Batch: 2, Critic Loss: -133.96206665039062, Generator Loss: 6.8138885498046875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 89, Batch: 3, Critic Loss: -109.41728210449219, Generator Loss: 31.701873779296875\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 89, Batch: 4, Critic Loss: -122.63613891601562, Generator Loss: 11.26087474822998\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 89, Batch: 5, Critic Loss: -178.7305908203125, Generator Loss: 25.417591094970703\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 90, Batch: 1, Critic Loss: -108.62169647216797, Generator Loss: 48.302650451660156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 90, Batch: 2, Critic Loss: -215.87417602539062, Generator Loss: -4.9148077964782715\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 90, Batch: 3, Critic Loss: -109.24232482910156, Generator Loss: 56.11579513549805\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 90, Batch: 4, Critic Loss: -180.19915771484375, Generator Loss: 31.71805191040039\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 90, Batch: 5, Critic Loss: -154.74053955078125, Generator Loss: 40.614654541015625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 91, Batch: 1, Critic Loss: -182.7160186767578, Generator Loss: 60.95713424682617\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 91, Batch: 2, Critic Loss: -149.77354431152344, Generator Loss: -10.049581527709961\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 91, Batch: 3, Critic Loss: -75.43307495117188, Generator Loss: 27.1949462890625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 91, Batch: 4, Critic Loss: -195.2802276611328, Generator Loss: -57.241859436035156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 91, Batch: 5, Critic Loss: -130.36676025390625, Generator Loss: -6.538479804992676\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 92, Batch: 1, Critic Loss: -112.5949478149414, Generator Loss: -10.017767906188965\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 92, Batch: 2, Critic Loss: -142.68017578125, Generator Loss: -111.21321868896484\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 92, Batch: 3, Critic Loss: -96.45382690429688, Generator Loss: -24.882726669311523\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 92, Batch: 4, Critic Loss: -158.55307006835938, Generator Loss: -18.28702163696289\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 92, Batch: 5, Critic Loss: -118.36650085449219, Generator Loss: 24.99323272705078\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 93, Batch: 1, Critic Loss: -114.82223510742188, Generator Loss: 53.766929626464844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 93, Batch: 2, Critic Loss: -107.45518493652344, Generator Loss: 5.115752220153809\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 93, Batch: 3, Critic Loss: -216.82516479492188, Generator Loss: 31.419574737548828\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 93, Batch: 4, Critic Loss: -76.83895111083984, Generator Loss: 32.19607925415039\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 93, Batch: 5, Critic Loss: -117.74175262451172, Generator Loss: 49.758201599121094\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 94, Batch: 1, Critic Loss: -101.71977233886719, Generator Loss: 56.345054626464844\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 94, Batch: 2, Critic Loss: -120.67115783691406, Generator Loss: 47.58776092529297\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 94, Batch: 3, Critic Loss: -167.83489990234375, Generator Loss: 72.02198791503906\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 94, Batch: 4, Critic Loss: -146.92820739746094, Generator Loss: 31.0231990814209\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 94, Batch: 5, Critic Loss: -145.8242950439453, Generator Loss: 70.79399871826172\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 95, Batch: 1, Critic Loss: -174.2255096435547, Generator Loss: 107.3802490234375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 95, Batch: 2, Critic Loss: -175.31887817382812, Generator Loss: 41.82289505004883\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 95, Batch: 3, Critic Loss: -88.53214263916016, Generator Loss: 18.19275665283203\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 95, Batch: 4, Critic Loss: -100.6176528930664, Generator Loss: 1.9824018478393555\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 95, Batch: 5, Critic Loss: -90.15156555175781, Generator Loss: 21.587890625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 96, Batch: 1, Critic Loss: -167.6608428955078, Generator Loss: 2.0774848461151123\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 96, Batch: 2, Critic Loss: -85.98231506347656, Generator Loss: 6.847434997558594\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 96, Batch: 3, Critic Loss: -102.75837707519531, Generator Loss: 25.199674606323242\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 96, Batch: 4, Critic Loss: -163.87705993652344, Generator Loss: 34.65159225463867\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 96, Batch: 5, Critic Loss: -134.3203125, Generator Loss: 15.079010009765625\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 97, Batch: 1, Critic Loss: -154.7423095703125, Generator Loss: -29.154016494750977\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 97, Batch: 2, Critic Loss: -98.46783447265625, Generator Loss: -2.6164002418518066\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 97, Batch: 3, Critic Loss: -106.94879913330078, Generator Loss: 24.124561309814453\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 97, Batch: 4, Critic Loss: -111.56112670898438, Generator Loss: 48.07950210571289\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 97, Batch: 5, Critic Loss: -186.884521484375, Generator Loss: 35.69579315185547\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 98, Batch: 1, Critic Loss: -102.40989685058594, Generator Loss: 44.523441314697266\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 98, Batch: 2, Critic Loss: -150.28814697265625, Generator Loss: 73.80012512207031\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 98, Batch: 3, Critic Loss: -121.37692260742188, Generator Loss: 63.503379821777344\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 98, Batch: 4, Critic Loss: -122.76412963867188, Generator Loss: 108.9300537109375\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 98, Batch: 5, Critic Loss: -136.1280517578125, Generator Loss: 32.585121154785156\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 99, Batch: 1, Critic Loss: -127.8666763305664, Generator Loss: 16.115877151489258\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 99, Batch: 2, Critic Loss: -117.37702941894531, Generator Loss: -28.072986602783203\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 99, Batch: 3, Critic Loss: -203.77894592285156, Generator Loss: -57.4826774597168\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 99, Batch: 4, Critic Loss: -157.79762268066406, Generator Loss: 12.475419998168945\n",
      "(10, 91, 91, 146)\n",
      "Epoch: 99, Batch: 5, Critic Loss: -79.87261199951172, Generator Loss: 36.97087097167969\n",
      "Epoch 1/14\n",
      "10/10 [==============================] - 2s 47ms/step - loss: 12.8252 - accuracy: 0.5000 - val_loss: 3.3815 - val_accuracy: 0.4167\n",
      "Epoch 2/14\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 9.3577 - accuracy: 0.6383 - val_loss: 4.0703 - val_accuracy: 0.6250\n",
      "Epoch 3/14\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 11.2752 - accuracy: 0.5213 - val_loss: 3.2567 - val_accuracy: 0.4167\n",
      "Epoch 4/14\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 11.5956 - accuracy: 0.4787 - val_loss: 3.6670 - val_accuracy: 0.4167\n",
      "Epoch 5/14\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 11.2433 - accuracy: 0.5213 - val_loss: 3.6329 - val_accuracy: 0.3750\n",
      "Epoch 6/14\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 8.4014 - accuracy: 0.5213 - val_loss: 3.0020 - val_accuracy: 0.3750\n",
      "Epoch 7/14\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 8.6007 - accuracy: 0.5851 - val_loss: 3.0073 - val_accuracy: 0.4167\n",
      "Epoch 8/14\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 8.7422 - accuracy: 0.5000 - val_loss: 3.6695 - val_accuracy: 0.4167\n",
      "Epoch 9/14\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 12.6755 - accuracy: 0.4362 - val_loss: 3.4940 - val_accuracy: 0.3750\n",
      "Epoch 10/14\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 10.7547 - accuracy: 0.4894 - val_loss: 3.6223 - val_accuracy: 0.5833\n",
      "Epoch 11/14\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 9.2441 - accuracy: 0.5638 - val_loss: 2.9077 - val_accuracy: 0.4167\n",
      "Epoch 12/14\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 12.7609 - accuracy: 0.5106 - val_loss: 4.3158 - val_accuracy: 0.3333\n",
      "Epoch 13/14\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 8.8389 - accuracy: 0.5426 - val_loss: 6.1198 - val_accuracy: 0.2917\n",
      "Epoch 14/14\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 10.3642 - accuracy: 0.5319 - val_loss: 5.9102 - val_accuracy: 0.2917\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "Test Loss: 3.4610188007354736, Test Accuracy: 0.48625001311302185\n",
      "Actual labels vs. Predicted labels:\n",
      "Image 1: Actual: 0, Predicted: 0\n",
      "Image 2: Actual: 0, Predicted: 0\n",
      "Image 3: Actual: 1, Predicted: 0\n",
      "Image 4: Actual: 0, Predicted: 0\n",
      "Image 5: Actual: 0, Predicted: 0\n",
      "Image 6: Actual: 1, Predicted: 0\n",
      "Image 7: Actual: 0, Predicted: 1\n",
      "Image 8: Actual: 0, Predicted: 0\n",
      "Image 9: Actual: 0, Predicted: 0\n",
      "Image 10: Actual: 1, Predicted: 0\n",
      "Image 11: Actual: 1, Predicted: 0\n",
      "Image 12: Actual: 1, Predicted: 0\n",
      "Image 13: Actual: 1, Predicted: 1\n",
      "Image 14: Actual: 0, Predicted: 0\n",
      "Image 15: Actual: 1, Predicted: 0\n",
      "Image 16: Actual: 1, Predicted: 0\n",
      "Image 17: Actual: 1, Predicted: 0\n",
      "Image 18: Actual: 0, Predicted: 0\n",
      "\n",
      "Average Test Loss over 1 iterations: 3.4610188007354736\n",
      "Average Test Accuracy over 1 iterations: 0.48625001311302185\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = 0\n",
    "total_loss = 0\n",
    "num_iterations = 1\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    ''' data training for GAN '''\n",
    "    ''' Choosing 50 random schizphrenia samples and store remaining 9 test'''\n",
    "\n",
    "        # GAN Training Data Selection\n",
    "    gan_train_ids = random.sample(met_requirement_schizophrenia_ids, 50)\n",
    "    gan_test_ids = [id for id in met_requirement_schizophrenia_ids if id not in gan_train_ids]\n",
    "\n",
    "    ''' data training for classifier '''\n",
    "    ''' Choosing 50 random schizphrenia samples from above and 50 new random control = 100 training'''\n",
    "    ''' test set here will have the 9 untouch schizophrenia from above plus 9 control that is not included in the training'''\n",
    "\n",
    "    # Classifier Training Data Selection\n",
    "    classifier_train_schizophrenia_ids = gan_train_ids  # Using the same set of schizophrenia IDs as GAN\n",
    "    classifier_train_control_ids = random.sample(met_requirement_control_ids, 50)\n",
    "    classifier_train_ids = classifier_train_schizophrenia_ids + classifier_train_control_ids\n",
    "\n",
    "    # Classifier Test Data Selection\n",
    "    # Ensuring the control IDs for testing are not included in the training set\n",
    "    remaining_control_ids_for_testing = [id for id in met_requirement_control_ids if id not in classifier_train_control_ids]\n",
    "    classifier_test_ids = gan_test_ids + random.sample(remaining_control_ids_for_testing, 9)\n",
    "\n",
    "    ''' File loading '''\n",
    "    # Specify the directory and file pattern\n",
    "    directory_path = '../4D'\n",
    "    file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "    # Construct the full path pattern\n",
    "    path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "    # Use glob to find all matching files\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "\n",
    "    ''' File loading for GAN Training '''\n",
    "    # Process only the 50 randomly selected schizophrenia files for GAN training\n",
    "    gan_image_data = []\n",
    "\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in gan_train_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "            gan_image_data.append(t1_data_collapsed)\n",
    "\n",
    "    print(f\"Total GAN training files processed: {len(gan_image_data)}\")\n",
    "\n",
    "    ''' File loading for Classifier Training and Testing '''\n",
    "    # Process files for both schizophrenia and control for classifier training and testing\n",
    "    classifier_image_data = []\n",
    "    classifier_labels = []  # 1 for schizophrenia, 0 for non-schizophrenia\n",
    "\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in classifier_train_ids or file_id in classifier_test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "\n",
    "            classifier_image_data.append(t1_data_collapsed)\n",
    "            classifier_labels.append(label)\n",
    "\n",
    "    print(f\"Total classifier training/testing files processed: {len(classifier_image_data)}\")\n",
    "    print(f\"Total labels processed: {len(classifier_labels)}\")\n",
    "\n",
    "    '''Determine the maximum z-dimension size '''\n",
    "    max_z_size = max(img.shape[2] for img in gan_image_data)\n",
    "    max_z_size_2 = max(img.shape[2] for img in classifier_image_data)\n",
    "\n",
    "    ''' normalization '''\n",
    "    image_data_normalized = [(img - np.min(img)) / (np.max(img) - np.min(img)) * 2 - 1 for img in gan_image_data]\n",
    "\n",
    "    image_data_normalized_2 = [(img - np.min(img)) / (np.max(img) - np.min(img)) * 2 - 1 for img in classifier_image_data]\n",
    "\n",
    "    ''' padding of images data '''\n",
    "    # Pad each image to have a consistent z-dimension size\n",
    "    padded_data = [np.pad(img, ((0, 0), (0, 0), (0, max_z_size - img.shape[2])), mode='constant') for img in image_data_normalized]\n",
    "\n",
    "    padded_data_2 = [np.pad(img, ((0, 0), (0, 0), (0, max_z_size_2 - img.shape[2])), mode='constant') for img in image_data_normalized_2]\n",
    "    \n",
    "    # Now convert the padded data list to a numpy array\n",
    "    padded_data_array = np.array(padded_data)\n",
    "    padded_data_array_2 = np.array(padded_data_2)\n",
    "\n",
    "    ''' loading the data for WGAN training '''\n",
    "    train_images = padded_data_array\n",
    "    # Define batch size\n",
    "    batch_size = 10\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images)).shuffle(len(train_images)).batch(batch_size)\n",
    "\n",
    "    \n",
    "    ''' setting up parameters'''\n",
    "    # Image shape and other parameters\n",
    "    img_shape = (84, 84, 72, 1)\n",
    "    z_dim = 100\n",
    "\n",
    "    # Create the generator and critic\n",
    "    generator = build_wgan_generator(z_dim)\n",
    "    critic = build_wgan_critic(img_shape)\n",
    "\n",
    "    critic_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "    generator_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    epochs = 10\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "\n",
    "    '''Train the WGAN-GP '''\n",
    "\n",
    "    critic_losses, generator_losses = train_wgan_gp(\n",
    "        generator, \n",
    "        critic, \n",
    "        train_dataset, \n",
    "        epochs, \n",
    "        z_dim,\n",
    "        lambda_gp, \n",
    "        critic_optimizer, \n",
    "        generator_optimizer\n",
    "    )\n",
    "    \n",
    "    ''' after training, retrieve the weights of the critic'''\n",
    "\n",
    "    checkpoint_dir = '/wgan_gp_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                    discriminator_optimizer=critic_optimizer,\n",
    "                                    generator=generator,\n",
    "                                    discriminator=critic)\n",
    "    \n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    ''' transfer learning '''\n",
    "    \n",
    "    ''' adding classification layers'''\n",
    "    new_model = Sequential()\n",
    "    for i in range(len(critic.layers)-1):  # Excluding the last layer\n",
    "        layer = critic.layers[i]\n",
    "        layer.trainable = False  # Freeze the layer\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Adding additional layers\n",
    "    new_model.add(Dropout(0.3))\n",
    "    new_model.add(Dense(1, activation='sigmoid', name = 'dense_6'))  # Sigmoid for binary classification\n",
    "\n",
    "    ''' Prepare data before training the classifer model, 2nd time train '''\n",
    "\n",
    "    ''' But we include control images to train this time around '''\n",
    "    # Resize each image in the padded data array\n",
    "    resized_images = [resize_image(img, (84, 84, 72)) for img in padded_data_2]\n",
    "\n",
    "    # Convert the resized data to a numpy array\n",
    "    resized_images_array = np.array(resized_images)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    # labels array corresponding to the images\n",
    "    labels_array = np.array(classifier_labels)\n",
    "    \n",
    "    ''' train classifier normally like in transfer-learning v2'''\n",
    "    # Split the data into training and evaluation sets (80% train, 20% eval)\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(resized_images_array, labels_array, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert to TensorFlow datasets with labels\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices((X_eval, y_eval)).batch(batch_size)\n",
    "\n",
    "    # Compile the model (if using a new instance of the model for each fold)\n",
    "    new_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = new_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=14,\n",
    "        validation_data=eval_dataset\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    ''' load test images to make test set'''\n",
    "    # Now proceed with loading and preprocessing the images for these IDs\n",
    "    test_image_data = []\n",
    "    test_labels = []\n",
    "    \n",
    "    test_ids = classifier_test_ids\n",
    "\n",
    "    # Loop through the matching files and filter based on test IDs\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "\n",
    "        # Process only if the ID is in the test set\n",
    "        if file_id in test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            # Ensure sufficient time dimension\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Collapse one of the axes by summing\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=1)\n",
    "\n",
    "            # Resize, normalize, and add dimension as done in the training data preparation\n",
    "            processed_image = resize_image(t1_data_collapsed, (84, 84, 72))\n",
    "            processed_image_normalized = (processed_image - np.min(processed_image)) / (np.max(processed_image) - np.min(processed_image)) * 2 - 1\n",
    "            processed_image_final = np.expand_dims(processed_image_normalized, axis=-1)\n",
    "\n",
    "            test_image_data.append(processed_image_final)\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "            test_labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    test_images_array = np.array(test_image_data)\n",
    "    test_labels_array = np.array(test_labels)\n",
    "\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    num_batches = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels_list = []\n",
    "    # Manually iterate over the test dataset\n",
    "    for images, labels in test_dataset:\n",
    "\n",
    "        print(f\"Current batch size: {len(images)}\")\n",
    "        # Make predictions\n",
    "        predictions = new_model.predict(images)\n",
    "\n",
    "        # Calculate loss for the batch\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n",
    "        test_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "        # Process predictions\n",
    "        predicted_labels_batch = tf.cast(tf.round(predictions), dtype=tf.int64)\n",
    "        predicted_labels_list.extend(predicted_labels_batch.numpy().flatten())\n",
    "        actual_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predicted_labels_batch), dtype=tf.float32))\n",
    "        test_accuracy += accuracy.numpy()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    \n",
    "    iteration_avg_loss = test_loss / num_batches\n",
    "    iteration_avg_accuracy = test_accuracy / num_batches\n",
    "\n",
    "    total_loss += iteration_avg_loss\n",
    "    total_accuracy += iteration_avg_accuracy\n",
    "    \n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {iteration_avg_loss}, Test Accuracy: {iteration_avg_accuracy}\")\n",
    "    print(\"Actual labels vs. Predicted labels:\")\n",
    "    for i in range(len(actual_labels)):\n",
    "        print(f\"Image {i+1}: Actual: {actual_labels[i]}, Predicted: {predicted_labels_list[i]}\")\n",
    "\n",
    "# Calculate and print the average loss and accuracy over all iterations\n",
    "average_test_loss = total_loss / num_iterations\n",
    "average_test_accuracy = total_accuracy / num_iterations\n",
    "print(f\"\\nAverage Test Loss over {num_iterations} iterations: {average_test_loss}\")\n",
    "print(f\"Average Test Accuracy over {num_iterations} iterations: {average_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Generate a single fake image\\nz = np.random.normal(0, 1, (1, z_dim))\\ngenerated_image = generator.predict(z)[0]  # [0] to get the single image from the batch\\n\\n# Get a single real image from the dataset\\nreal_images = next(iter(train_dataset))[0]  # Assuming the dataset yields only images\\n\\n# Take the first real image from the batch for comparison\\nreal_image = real_images[0]\\n\\n# Plot the real and fake images side by side\\nplt.figure(figsize=(10, 5))\\n\\n# Plot real image\\nplt.subplot(1, 2, 1)\\nplt.imshow(real_image[:, :, 10, 0])  # Adjust indexing and color map as needed\\nplt.title('Real Image')\\nplt.axis('off')\\n\\n# Plot fake image\\nplt.subplot(1, 2, 2)\\nplt.imshow(generated_image[:, :, 10, 0])  # Adjust indexing and color map as needed\\nplt.title('Generated Image')\\nplt.axis('off')\\n\\nplt.tight_layout()\\nplt.show() \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Generate a single fake image\n",
    "z = np.random.normal(0, 1, (1, z_dim))\n",
    "generated_image = generator.predict(z)[0]  # [0] to get the single image from the batch\n",
    "\n",
    "# Get a single real image from the dataset\n",
    "real_images = next(iter(train_dataset))[0]  # Assuming the dataset yields only images\n",
    "\n",
    "# Take the first real image from the batch for comparison\n",
    "real_image = real_images[0]\n",
    "\n",
    "# Plot the real and fake images side by side\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot real image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(real_image[:, :, 10, 0])  # Adjust indexing and color map as needed\n",
    "plt.title('Real Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot fake image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(generated_image[:, :, 10, 0])  # Adjust indexing and color map as needed\n",
    "plt.title('Generated Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.save('wgan_generator_model_v3.h5')\n",
    "#critic.save('wgan_critic_model_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b815ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96a4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
